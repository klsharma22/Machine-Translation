{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9590096b-77e9-4f1e-ba93-874e043953e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\satya\\anaconda3\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\satya\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import BertTokenizer, BertModel,AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "from flair.embeddings import WordEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tensorflow as tf \n",
    "import os\n",
    "import json\n",
    "import pyarrow\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9e2c42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1650 Ti'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad6f2ab3-655e-456d-a96e-e1fd3cb05b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>French</th>\n",
       "      <th>Citation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Marche.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>En route !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Bouge !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>He's good at telling jokes.</td>\n",
       "      <td>Il est bon pour raconter des blagues.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>He's handsome and charming.</td>\n",
       "      <td>Il est beau et a du charme.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>He's much younger than Tom.</td>\n",
       "      <td>Il est beaucoup plus jeune que Tom.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>He's my old drinking buddy.</td>\n",
       "      <td>C'est mon vieux compagnon de beuverie.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>He's now a college student.</td>\n",
       "      <td>Il est maintenant étudiant à la fac.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           English                                  French  \\\n",
       "0                              Go.                                    Va !   \n",
       "1                              Go.                                 Marche.   \n",
       "2                              Go.                              En route !   \n",
       "3                              Go.                                 Bouge !   \n",
       "4                              Hi.                                 Salut !   \n",
       "...                            ...                                     ...   \n",
       "99995  He's good at telling jokes.   Il est bon pour raconter des blagues.   \n",
       "99996  He's handsome and charming.             Il est beau et a du charme.   \n",
       "99997  He's much younger than Tom.     Il est beaucoup plus jeune que Tom.   \n",
       "99998  He's my old drinking buddy.  C'est mon vieux compagnon de beuverie.   \n",
       "99999  He's now a college student.    Il est maintenant étudiant à la fac.   \n",
       "\n",
       "                                                Citation  \n",
       "0      CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "1      CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "2      CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "3      CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "4      CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
       "...                                                  ...  \n",
       "99995  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "99996  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "99997  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "99998  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "99999  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "\n",
       "[100000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_table('fra.txt', header= None)\n",
    "data.rename(columns= {0: 'English', 1: 'French', 2: 'Citation'}, inplace= True)\n",
    "data = data[:100000]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a157aba8-9988-4afe-bb17-349d365b3286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go',\n",
       " 'Go',\n",
       " 'Go',\n",
       " 'Go',\n",
       " 'Hi',\n",
       " 'Hi',\n",
       " 'Run',\n",
       " 'Run',\n",
       " 'Run',\n",
       " 'Run',\n",
       " 'Run',\n",
       " 'Run',\n",
       " 'Run',\n",
       " 'Run',\n",
       " 'Run',\n",
       " 'Run',\n",
       " 'Run',\n",
       " 'Run',\n",
       " 'Run',\n",
       " 'Run',\n",
       " 'Run',\n",
       " 'Run',\n",
       " 'Who',\n",
       " 'Wow',\n",
       " 'Wow',\n",
       " 'Wow',\n",
       " 'Duck',\n",
       " 'Duck',\n",
       " 'Duck',\n",
       " 'Fire',\n",
       " 'Help',\n",
       " 'Hide',\n",
       " 'Hide',\n",
       " 'Jump',\n",
       " 'Jump',\n",
       " 'Stop',\n",
       " 'Stop',\n",
       " 'Stop',\n",
       " 'Wait',\n",
       " 'Wait',\n",
       " 'Wait',\n",
       " 'Wait',\n",
       " 'Wait',\n",
       " 'Wait',\n",
       " 'Wait',\n",
       " 'Begin',\n",
       " 'Begin',\n",
       " 'Go on',\n",
       " 'Go on',\n",
       " 'Go on',\n",
       " 'Hello',\n",
       " 'Hello',\n",
       " 'Hello',\n",
       " 'Hello',\n",
       " 'Hello',\n",
       " 'Hello',\n",
       " 'I see',\n",
       " 'I see',\n",
       " 'I try',\n",
       " 'I won',\n",
       " 'I won',\n",
       " 'I won',\n",
       " 'Oh no',\n",
       " 'Relax',\n",
       " 'Relax',\n",
       " 'Relax',\n",
       " 'Relax',\n",
       " 'Relax',\n",
       " 'Relax',\n",
       " 'Relax',\n",
       " 'Relax',\n",
       " 'Relax',\n",
       " 'Relax',\n",
       " 'Relax',\n",
       " 'Relax',\n",
       " 'Smile',\n",
       " 'Smile',\n",
       " 'Smile',\n",
       " 'Sorry',\n",
       " 'Attack',\n",
       " 'Attack',\n",
       " 'Attack',\n",
       " 'Attack',\n",
       " 'Buy it',\n",
       " 'Buy it',\n",
       " 'Buy it',\n",
       " 'Buy it',\n",
       " 'Cheers',\n",
       " 'Cheers',\n",
       " 'Cheers',\n",
       " 'Cheers',\n",
       " 'Eat it',\n",
       " 'Eat it',\n",
       " 'Exhale',\n",
       " 'Get up',\n",
       " 'Get up',\n",
       " 'Get up',\n",
       " 'Go now',\n",
       " 'Go now',\n",
       " 'Go now',\n",
       " 'Got it',\n",
       " 'Got it',\n",
       " 'Got it',\n",
       " 'Got it',\n",
       " 'Got it',\n",
       " 'Got it',\n",
       " 'Hop in',\n",
       " 'Hop in',\n",
       " 'Hug me',\n",
       " 'Hug me',\n",
       " 'I fell',\n",
       " 'I fell',\n",
       " 'I fled',\n",
       " 'I hunt',\n",
       " 'I knit',\n",
       " 'I know',\n",
       " 'I left',\n",
       " 'I left',\n",
       " 'I lied',\n",
       " 'I lost',\n",
       " 'I paid',\n",
       " 'I paid',\n",
       " 'I paid',\n",
       " 'I pass',\n",
       " 'I quit',\n",
       " 'I quit',\n",
       " 'I swim',\n",
       " 'Im 19',\n",
       " 'Im OK',\n",
       " 'Im OK',\n",
       " 'Inhale',\n",
       " 'Listen',\n",
       " 'No way',\n",
       " 'No way',\n",
       " 'No way',\n",
       " 'No way',\n",
       " 'No way',\n",
       " 'No way',\n",
       " 'No way',\n",
       " 'No way',\n",
       " 'No way',\n",
       " 'No way',\n",
       " 'No way',\n",
       " 'No way',\n",
       " 'Really',\n",
       " 'Really',\n",
       " 'Really',\n",
       " 'Really',\n",
       " 'Really',\n",
       " 'Thanks',\n",
       " 'Thanks',\n",
       " 'Thanks',\n",
       " 'Thanks',\n",
       " 'Try it',\n",
       " 'Try it',\n",
       " 'Try it',\n",
       " 'We try',\n",
       " 'We won',\n",
       " 'We won',\n",
       " 'We won',\n",
       " 'We won',\n",
       " 'We won',\n",
       " 'Ask Tom',\n",
       " 'Ask him',\n",
       " 'Awesome',\n",
       " 'Awesome',\n",
       " 'Awesome',\n",
       " 'Awesome',\n",
       " 'Be calm',\n",
       " 'Be calm',\n",
       " 'Be calm',\n",
       " 'Be cool',\n",
       " 'Be fair',\n",
       " 'Be fair',\n",
       " 'Be fair',\n",
       " 'Be fair',\n",
       " 'Be fair',\n",
       " 'Be fair',\n",
       " 'Be fair',\n",
       " 'Be fair',\n",
       " 'Be fair',\n",
       " 'Be kind',\n",
       " 'Be nice',\n",
       " 'Be nice',\n",
       " 'Be nice',\n",
       " 'Be nice',\n",
       " 'Be nice',\n",
       " 'Be nice',\n",
       " 'Beat it',\n",
       " 'Beat it',\n",
       " 'Beat it',\n",
       " 'Beat it',\n",
       " 'Beat it',\n",
       " 'Beat it',\n",
       " 'Beat it',\n",
       " 'Beat it',\n",
       " 'Beat it',\n",
       " 'Beat it',\n",
       " 'Beat it',\n",
       " 'Beat it',\n",
       " 'Beat it',\n",
       " 'Beat it',\n",
       " 'Beat it',\n",
       " 'Beat it',\n",
       " 'Beat it',\n",
       " 'Beat it',\n",
       " 'Beat it',\n",
       " 'Burn it',\n",
       " 'Burn it',\n",
       " 'Burn it',\n",
       " 'Burn it',\n",
       " 'Bury it',\n",
       " 'Bury it',\n",
       " 'Bury it',\n",
       " 'Bury it',\n",
       " 'Call me',\n",
       " 'Call me',\n",
       " 'Call us',\n",
       " 'Call us',\n",
       " 'Come in',\n",
       " 'Come in',\n",
       " 'Come in',\n",
       " 'Come in',\n",
       " 'Come on',\n",
       " 'Come on',\n",
       " 'Come on',\n",
       " 'Come on',\n",
       " 'Come on',\n",
       " 'Come on',\n",
       " 'Come on',\n",
       " 'Come on',\n",
       " 'Come on',\n",
       " 'Come on',\n",
       " 'Come on',\n",
       " 'Come on',\n",
       " 'Drop it',\n",
       " 'Drop it',\n",
       " 'Drop it',\n",
       " 'Drop it',\n",
       " 'Fold it',\n",
       " 'Fold it',\n",
       " 'Fold it',\n",
       " 'Fold it',\n",
       " 'Get Tom',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Get out',\n",
       " 'Go away',\n",
       " 'Go away',\n",
       " 'Go away',\n",
       " 'Go away',\n",
       " 'Go away',\n",
       " 'Go away',\n",
       " 'Go away',\n",
       " 'Go away',\n",
       " 'Go away',\n",
       " 'Go away',\n",
       " 'Go away',\n",
       " 'Go away',\n",
       " 'Go away',\n",
       " 'Go away',\n",
       " 'Go away',\n",
       " 'Go away',\n",
       " 'Go away',\n",
       " 'Go away',\n",
       " 'Go away',\n",
       " 'Go away',\n",
       " 'Go away',\n",
       " 'Go back',\n",
       " 'Go home',\n",
       " 'Go home',\n",
       " 'Go home',\n",
       " 'Go home',\n",
       " 'Go slow',\n",
       " 'Go slow',\n",
       " 'Goodbye',\n",
       " 'Goodbye',\n",
       " 'Goodbye',\n",
       " 'Goodbye',\n",
       " 'Goodbye',\n",
       " 'Hang on',\n",
       " 'Hang on',\n",
       " 'Hang on',\n",
       " 'Hang on',\n",
       " 'Hang on',\n",
       " 'Hang on',\n",
       " 'He left',\n",
       " 'He runs',\n",
       " 'Help me',\n",
       " 'Help me',\n",
       " 'Help me',\n",
       " 'Help me',\n",
       " 'Help me',\n",
       " 'Help us',\n",
       " 'Help us',\n",
       " 'Hold it',\n",
       " 'Hold it',\n",
       " 'Hold it',\n",
       " 'Hold it',\n",
       " 'Hold on',\n",
       " 'Hold on',\n",
       " 'Hold up',\n",
       " 'Hold up',\n",
       " 'How sad',\n",
       " 'Hug Tom',\n",
       " 'I agree',\n",
       " 'I cried',\n",
       " 'I dozed',\n",
       " 'I dozed',\n",
       " 'I drive',\n",
       " 'I drove',\n",
       " 'I fired',\n",
       " 'I froze',\n",
       " 'I froze',\n",
       " 'I smoke',\n",
       " 'I snore',\n",
       " 'I stink',\n",
       " 'I stood',\n",
       " 'I stood',\n",
       " 'I swore',\n",
       " 'I swore',\n",
       " 'I tried',\n",
       " 'I tried',\n",
       " 'I tried',\n",
       " 'I waved',\n",
       " 'Ill go',\n",
       " 'Im Tom',\n",
       " 'Im fat',\n",
       " 'Im fat',\n",
       " 'Im fit',\n",
       " 'Im hit',\n",
       " 'Im hit',\n",
       " 'Im ill',\n",
       " 'Im mad',\n",
       " 'Im mad',\n",
       " 'Im mad',\n",
       " 'Im sad',\n",
       " 'Im sad',\n",
       " 'Im sad',\n",
       " 'Im shy',\n",
       " 'Im wet',\n",
       " 'Im wet',\n",
       " 'Its me',\n",
       " 'Join us',\n",
       " 'Join us',\n",
       " 'Keep it',\n",
       " 'Keep it',\n",
       " 'Kick it',\n",
       " 'Kick it',\n",
       " 'Kill it',\n",
       " 'Kill it',\n",
       " 'Kill it',\n",
       " 'Kill it',\n",
       " 'Kiss me',\n",
       " 'Kiss me',\n",
       " 'Lie low',\n",
       " 'Lie low',\n",
       " 'Lock it',\n",
       " 'Lock it',\n",
       " 'Lock it',\n",
       " 'Lock it',\n",
       " 'Look up',\n",
       " 'Look up',\n",
       " 'Me too',\n",
       " 'Me too',\n",
       " 'Move on',\n",
       " 'Move on',\n",
       " 'Open it',\n",
       " 'Open it',\n",
       " 'Open it',\n",
       " 'Open it',\n",
       " 'Open up',\n",
       " 'Open up',\n",
       " 'Pair up',\n",
       " 'Perfect',\n",
       " 'Perfect',\n",
       " 'Perfect',\n",
       " 'Pull it',\n",
       " 'Pull it',\n",
       " 'Push it',\n",
       " 'Push it',\n",
       " 'Push it',\n",
       " 'Push it',\n",
       " 'Push it',\n",
       " 'See you',\n",
       " 'See you',\n",
       " 'See you',\n",
       " 'See you',\n",
       " 'See you',\n",
       " 'See you',\n",
       " 'Show me',\n",
       " 'Show me',\n",
       " 'Shut up',\n",
       " 'Shut up',\n",
       " 'Shut up',\n",
       " 'Shut up',\n",
       " 'Shut up',\n",
       " 'Shut up',\n",
       " 'Shut up',\n",
       " 'Shut up',\n",
       " 'Sign up',\n",
       " 'Sign up',\n",
       " 'Skip it',\n",
       " 'Skip it',\n",
       " 'So long',\n",
       " 'Take it',\n",
       " 'Take it',\n",
       " 'Take it',\n",
       " 'Take it',\n",
       " 'Take it',\n",
       " 'Take it',\n",
       " 'Tell me',\n",
       " 'Tell me',\n",
       " 'Tom won',\n",
       " 'Wake up',\n",
       " 'Wake up',\n",
       " 'Wake up',\n",
       " 'Wake up',\n",
       " 'Wake up',\n",
       " 'Wash up',\n",
       " 'Wash up',\n",
       " 'We care',\n",
       " 'We know',\n",
       " 'We know',\n",
       " 'We lost',\n",
       " 'We lost',\n",
       " 'We lost',\n",
       " 'We lost',\n",
       " 'We lost',\n",
       " 'We lost',\n",
       " 'We lost',\n",
       " 'We lost',\n",
       " 'We lost',\n",
       " 'We lost',\n",
       " 'Welcome',\n",
       " 'Welcome',\n",
       " 'Who ran',\n",
       " 'Who won',\n",
       " 'Who won',\n",
       " 'You run',\n",
       " 'You win',\n",
       " 'Aim high',\n",
       " 'Aim high',\n",
       " 'Am I fat',\n",
       " 'Am I fat',\n",
       " 'Ask them',\n",
       " 'Ask them',\n",
       " 'Back off',\n",
       " 'Back off',\n",
       " 'Back off',\n",
       " 'Back off',\n",
       " 'Back off',\n",
       " 'Back off',\n",
       " 'Back off',\n",
       " 'Back off',\n",
       " 'Be a man',\n",
       " 'Be a man',\n",
       " 'Be brave',\n",
       " 'Be brief',\n",
       " 'Be brief',\n",
       " 'Be brief',\n",
       " 'Be brief',\n",
       " 'Be brief',\n",
       " 'Be brief',\n",
       " 'Be still',\n",
       " 'Be still',\n",
       " 'Be still',\n",
       " 'Buzz off',\n",
       " 'Call Tom',\n",
       " 'Call Tom',\n",
       " 'Can I go',\n",
       " 'Cheer up',\n",
       " 'Cheer up',\n",
       " 'Cool off',\n",
       " 'Cover me',\n",
       " 'Cuff him',\n",
       " 'Drive on',\n",
       " 'Drive on',\n",
       " 'Drive on',\n",
       " 'Drive on',\n",
       " 'Find Tom',\n",
       " 'Find Tom',\n",
       " 'Fix this',\n",
       " 'Fix this',\n",
       " 'Get away',\n",
       " 'Get away',\n",
       " 'Get away',\n",
       " 'Get away',\n",
       " 'Get away',\n",
       " 'Get away',\n",
       " 'Get away',\n",
       " 'Get away',\n",
       " 'Get away',\n",
       " 'Get away',\n",
       " 'Get away',\n",
       " 'Get away',\n",
       " 'Get away',\n",
       " 'Get away',\n",
       " 'Get away',\n",
       " 'Get away',\n",
       " 'Get away',\n",
       " 'Get away',\n",
       " 'Get away',\n",
       " 'Get away',\n",
       " 'Get away',\n",
       " 'Get away',\n",
       " 'Get away',\n",
       " 'Get away',\n",
       " 'Get away',\n",
       " 'Get away',\n",
       " 'Get away',\n",
       " 'Get down',\n",
       " 'Get down',\n",
       " 'Get down',\n",
       " 'Get down',\n",
       " 'Get down',\n",
       " 'Get down',\n",
       " 'Get down',\n",
       " 'Get down',\n",
       " 'Get down',\n",
       " 'Get down',\n",
       " 'Get down',\n",
       " 'Get lost',\n",
       " 'Get lost',\n",
       " 'Get lost',\n",
       " 'Get lost',\n",
       " 'Get lost',\n",
       " 'Get lost',\n",
       " 'Get lost',\n",
       " 'Get lost',\n",
       " 'Get lost',\n",
       " 'Get lost',\n",
       " 'Get lost',\n",
       " 'Get lost',\n",
       " 'Get lost',\n",
       " 'Get lost',\n",
       " 'Get lost',\n",
       " 'Get lost',\n",
       " 'Get lost',\n",
       " 'Get real',\n",
       " 'Get real',\n",
       " 'Go ahead',\n",
       " 'Go ahead',\n",
       " 'Go ahead',\n",
       " 'Go ahead',\n",
       " 'Go ahead',\n",
       " 'Go ahead',\n",
       " 'Go ahead',\n",
       " 'Go ahead',\n",
       " 'Go ahead',\n",
       " 'Go ahead',\n",
       " 'Go ahead',\n",
       " 'Go ahead',\n",
       " 'Go ahead',\n",
       " 'Go ahead',\n",
       " 'Go ahead',\n",
       " 'Go ahead',\n",
       " 'Go ahead',\n",
       " 'Go ahead',\n",
       " 'Go ahead',\n",
       " 'Go ahead',\n",
       " 'Go ahead',\n",
       " 'Go ahead',\n",
       " 'Go ahead',\n",
       " 'Go ahead',\n",
       " 'Go ahead',\n",
       " 'Go ahead',\n",
       " 'Go ahead',\n",
       " 'Good job',\n",
       " 'Good job',\n",
       " 'Good job',\n",
       " 'Good job',\n",
       " 'Grab Tom',\n",
       " 'Grab Tom',\n",
       " 'Grab him',\n",
       " 'Grab him',\n",
       " 'Grab him',\n",
       " 'Have fun',\n",
       " 'Have fun',\n",
       " 'Have fun',\n",
       " 'Have fun',\n",
       " 'He spoke',\n",
       " 'He spoke',\n",
       " 'He spoke',\n",
       " 'He tries',\n",
       " 'Hes wet',\n",
       " 'Help Tom',\n",
       " 'Help Tom',\n",
       " 'Help Tom',\n",
       " 'Hi guys',\n",
       " 'How cute',\n",
       " 'How cute',\n",
       " 'How cute',\n",
       " 'How deep',\n",
       " 'How nice',\n",
       " 'How nice',\n",
       " 'How nice',\n",
       " 'How nice',\n",
       " 'How nice',\n",
       " 'How rude',\n",
       " 'How wise',\n",
       " 'Hurry up',\n",
       " 'Hurry up',\n",
       " 'Hurry up',\n",
       " 'Hurry up',\n",
       " 'Hurry up',\n",
       " 'Hurry up',\n",
       " 'I am Tom',\n",
       " 'I cursed',\n",
       " 'I did OK',\n",
       " 'I did OK',\n",
       " 'I did it',\n",
       " 'I did it',\n",
       " 'I failed',\n",
       " 'I forgot',\n",
       " 'I get it',\n",
       " 'I goofed',\n",
       " 'I got it',\n",
       " 'I got it',\n",
       " 'I helped',\n",
       " 'I jumped',\n",
       " 'I looked',\n",
       " 'I moaned',\n",
       " 'I nodded',\n",
       " 'I obeyed',\n",
       " 'I phoned',\n",
       " 'I phoned',\n",
       " 'I refuse',\n",
       " 'I refuse',\n",
       " 'I rested',\n",
       " 'I rested',\n",
       " 'I saw it',\n",
       " 'I saw it',\n",
       " 'I sighed',\n",
       " 'I smiled',\n",
       " 'I stayed',\n",
       " 'I stayed',\n",
       " 'I talked',\n",
       " 'I use it',\n",
       " 'I use it',\n",
       " 'I use it',\n",
       " 'Ill pay',\n",
       " 'Ill pay',\n",
       " 'Ill try',\n",
       " 'Ill try',\n",
       " 'Im back',\n",
       " 'Im back',\n",
       " 'Im bald',\n",
       " 'Im busy',\n",
       " 'Im busy',\n",
       " 'Im calm',\n",
       " 'Im cold',\n",
       " 'Im cool',\n",
       " 'Im cool',\n",
       " 'Im deaf',\n",
       " 'Im deaf',\n",
       " 'Im done',\n",
       " 'Im fair',\n",
       " 'Im fair',\n",
       " 'Im fair',\n",
       " 'Im fast',\n",
       " 'Im fine',\n",
       " 'Im fine',\n",
       " 'Im fine',\n",
       " 'Im free',\n",
       " 'Im free',\n",
       " 'Im free',\n",
       " 'Im full',\n",
       " 'Im full',\n",
       " 'Im game',\n",
       " 'Im game',\n",
       " 'Im glad',\n",
       " 'Im good',\n",
       " 'Im good',\n",
       " 'Im home',\n",
       " 'Im late',\n",
       " 'Im lazy',\n",
       " 'Im lazy',\n",
       " 'Im lazy',\n",
       " 'Im lazy',\n",
       " 'Im lost',\n",
       " 'Im lost',\n",
       " 'Im okay',\n",
       " 'Im okay',\n",
       " 'Im rich',\n",
       " 'Im safe',\n",
       " 'Im sick',\n",
       " 'Im sure',\n",
       " 'Im sure',\n",
       " 'Im sure',\n",
       " 'Im sure',\n",
       " 'Im tall',\n",
       " 'Im thin',\n",
       " 'Im tidy',\n",
       " 'Im tidy',\n",
       " 'Im ugly',\n",
       " 'Im ugly',\n",
       " 'Im ugly',\n",
       " 'Im weak',\n",
       " 'Im well',\n",
       " 'Im well',\n",
       " 'Ive won',\n",
       " 'Ive won',\n",
       " 'It helps',\n",
       " 'It hurts',\n",
       " 'It works',\n",
       " 'It works',\n",
       " 'Its Tom',\n",
       " 'Its fun',\n",
       " 'Its fun',\n",
       " 'Its his',\n",
       " 'Its his',\n",
       " 'Its hot',\n",
       " 'Its new',\n",
       " 'Its new',\n",
       " 'Its odd',\n",
       " 'Its red',\n",
       " 'Its sad',\n",
       " 'Keep out',\n",
       " 'Keep out',\n",
       " 'Kill Tom',\n",
       " 'Kill Tom',\n",
       " 'Kiss Tom',\n",
       " 'Leave it',\n",
       " 'Leave it',\n",
       " 'Leave it',\n",
       " 'Leave it',\n",
       " 'Leave it',\n",
       " 'Leave me',\n",
       " 'Leave us',\n",
       " 'Leave us',\n",
       " 'Lets go',\n",
       " 'Lets go',\n",
       " 'Lets go',\n",
       " 'Lets go',\n",
       " 'Lets go',\n",
       " 'Look out',\n",
       " 'Look out',\n",
       " 'Look out',\n",
       " 'Look out',\n",
       " 'Look out',\n",
       " 'Look out',\n",
       " 'Look out',\n",
       " 'Look out',\n",
       " 'Look out',\n",
       " 'Look out',\n",
       " 'Marry me',\n",
       " 'Marry me',\n",
       " 'May I go',\n",
       " 'May I go',\n",
       " 'May I go',\n",
       " 'Nice try',\n",
       " 'Now stop',\n",
       " 'Now stop',\n",
       " 'Prove it',\n",
       " 'Prove it',\n",
       " 'Run away',\n",
       " 'Run away',\n",
       " 'Run away',\n",
       " 'Run away',\n",
       " 'Save Tom',\n",
       " 'Save Tom',\n",
       " 'Say what',\n",
       " 'She came',\n",
       " 'She died',\n",
       " 'She left',\n",
       " 'She runs',\n",
       " 'Sit down',\n",
       " 'Sit down',\n",
       " 'Sit down',\n",
       " 'Sit down',\n",
       " 'Sit down',\n",
       " 'Sit here',\n",
       " 'Sit here',\n",
       " 'Speak up',\n",
       " 'Speak up',\n",
       " 'Speak up',\n",
       " 'Speed up',\n",
       " 'Speed up',\n",
       " 'Stand up',\n",
       " 'Stop Tom',\n",
       " 'Stop Tom',\n",
       " 'Take Tom',\n",
       " 'Taste it',\n",
       " 'Taste it',\n",
       " 'Taste it',\n",
       " 'Taste it',\n",
       " 'Tell Tom',\n",
       " 'Tell Tom',\n",
       " 'Terrific',\n",
       " 'Terrific',\n",
       " 'Terrific',\n",
       " 'Terrific',\n",
       " 'Terrific',\n",
       " 'Terrific',\n",
       " 'Terrific',\n",
       " 'Terrific',\n",
       " 'Terrific',\n",
       " 'Terrific',\n",
       " 'Terrific',\n",
       " 'Terrific',\n",
       " 'Terrific',\n",
       " 'Terrific',\n",
       " 'Terrific',\n",
       " 'Terrific',\n",
       " 'Terrific',\n",
       " 'They won',\n",
       " 'They won',\n",
       " 'They won',\n",
       " 'They won',\n",
       " 'Tom came',\n",
       " 'Tom died',\n",
       " 'Tom knew',\n",
       " 'Tom left',\n",
       " 'Tom left',\n",
       " 'Tom lied',\n",
       " 'Tom lies',\n",
       " 'Tom lost',\n",
       " 'Tom paid',\n",
       " 'Tom paid',\n",
       " 'Tom went',\n",
       " 'Toms up',\n",
       " 'Too late',\n",
       " 'Touch it',\n",
       " 'Touch it',\n",
       " 'Touch it',\n",
       " 'Touch it',\n",
       " 'Trust me',\n",
       " 'Trust me',\n",
       " 'Trust me',\n",
       " 'Trust me',\n",
       " 'Trust me',\n",
       " 'Try some',\n",
       " 'Try some',\n",
       " 'Try some',\n",
       " 'Try this',\n",
       " 'Try this',\n",
       " 'Try this',\n",
       " 'Use this',\n",
       " 'Use this',\n",
       " 'Use this',\n",
       " 'Use this',\n",
       " 'Warn Tom',\n",
       " 'Warn Tom',\n",
       " 'Watch me',\n",
       " 'Watch me',\n",
       " 'Watch us',\n",
       " 'Watch us',\n",
       " 'We agree',\n",
       " 'Well go',\n",
       " 'Were OK',\n",
       " 'What for',\n",
       " 'What for',\n",
       " 'What fun',\n",
       " 'What fun',\n",
       " 'Who am I',\n",
       " 'Who came',\n",
       " 'Who died',\n",
       " 'Who fell',\n",
       " 'Who lost',\n",
       " 'Who paid',\n",
       " 'Who quit',\n",
       " 'Whos he',\n",
       " 'Write me',\n",
       " 'Write me',\n",
       " 'You lost',\n",
       " 'You lost',\n",
       " 'You lost',\n",
       " 'After you',\n",
       " 'After you',\n",
       " 'After you',\n",
       " 'After you',\n",
       " 'Aim Fire',\n",
       " 'Am I cute',\n",
       " 'Am I late',\n",
       " 'Answer me',\n",
       " 'Be honest',\n",
       " 'Be honest',\n",
       " 'Be honest',\n",
       " 'Be seated',\n",
       " 'Be seated',\n",
       " 'Be seated',\n",
       " 'Be strong',\n",
       " 'Birds fly',\n",
       " 'Bless you',\n",
       " 'Call home',\n",
       " 'Calm down',\n",
       " 'Calm down',\n",
       " 'Calm down',\n",
       " 'Calm down',\n",
       " 'Calm down',\n",
       " 'Calm down',\n",
       " 'Can we go',\n",
       " 'Can we go',\n",
       " 'Can we go',\n",
       " 'Catch Tom',\n",
       " 'Catch Tom',\n",
       " 'Catch him',\n",
       " 'Catch him',\n",
       " 'Catch him',\n",
       " 'Chill out',\n",
       " 'Chill out',\n",
       " 'Choose me',\n",
       " 'Come back',\n",
       " 'Come back',\n",
       " 'Come here',\n",
       " 'Come here',\n",
       " 'Come over',\n",
       " 'Come over',\n",
       " 'Come over',\n",
       " 'Come over',\n",
       " 'Come over',\n",
       " 'Come soon',\n",
       " 'Come soon',\n",
       " 'Cool down',\n",
       " 'Did I win',\n",
       " 'Did I win',\n",
       " 'Did I win',\n",
       " 'Do it now',\n",
       " 'Dogs bark',\n",
       " 'Dogs bark',\n",
       " 'Dont ask',\n",
       " 'Dont cry',\n",
       " 'Dont die',\n",
       " 'Dont die',\n",
       " 'Dont lie',\n",
       " 'Dont lie',\n",
       " 'Dont run',\n",
       " 'Dont run',\n",
       " 'Excuse me',\n",
       " 'Excuse me',\n",
       " 'Excuse me',\n",
       " 'Excuse me',\n",
       " 'Excuse me',\n",
       " 'Excuse me',\n",
       " 'Excuse me',\n",
       " 'Fantastic',\n",
       " 'Fantastic',\n",
       " 'Fantastic',\n",
       " 'Feel this',\n",
       " 'Feel this',\n",
       " 'Feel this',\n",
       " 'Feel this',\n",
       " 'Film this',\n",
       " 'Film this',\n",
       " 'Follow me',\n",
       " 'Follow us',\n",
       " 'Follow us',\n",
       " 'Forget it',\n",
       " 'Forget it',\n",
       " 'Forget it',\n",
       " 'Forget it',\n",
       " 'Forget it',\n",
       " 'Forget it',\n",
       " 'Forget it',\n",
       " 'Forget it',\n",
       " 'Forget it',\n",
       " 'Forget it',\n",
       " 'Forget it',\n",
       " 'Forget me',\n",
       " 'Forget me',\n",
       " 'Get a job',\n",
       " 'Get a job',\n",
       " 'Get a job',\n",
       " 'Get a job',\n",
       " 'Get a saw',\n",
       " 'Get going',\n",
       " 'Get going',\n",
       " 'Get going',\n",
       " 'Get going',\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r\"[!'#$%&()*+,-./:;<=>?@[\\]^`{|}~“”‘’«»‹›„‚–—…·•¡¿’\\\"\\']\"\n",
    "\n",
    "eng_sent, french_sent = [], []\n",
    "\n",
    "for e in range(len(data['English'])):\n",
    "    eng_sent.append(re.sub(pattern, \"\", data['English'][e]))\n",
    "    french_sent.append(re.sub(pattern, \"\", data['French'][e]))\n",
    "#eng_sent[229801]\n",
    "eng_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa56c84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(len(eng_sent))\n",
    "print(len(french_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92f1d228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.038\n",
      "13.204000000000008\n"
     ]
    }
   ],
   "source": [
    "print(100 - (len(set(eng_sent))/len(eng_sent))*100)\n",
    "print(100 - (len(set(french_sent))/len(french_sent))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b6308ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_sent_unique = list(set(eng_sent))\n",
    "french_sent_unique = list(set(french_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6bafb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "bert_small_tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-small-uncased\")\n",
    "bert_small_model = AutoModel.from_pretrained(\"nlpaueb/legal-bert-small-uncased\")\n",
    "bert_tiny_tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "Bert_tiny_model = AutoModel.from_pretrained(\"prajjwal1/bert-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3d71d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bert_tiny_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e3aa447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embedding2(batch_tokens, max_len, model, tokenizer):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #model = model.to(device)\n",
    "\n",
    "    # Reduce the batch size if the input is too large for GPU memory\n",
    "    batch_size = len(batch_tokens)\n",
    "    max_batch_size = 32  # You can adjust this value based on your GPU memory capacity\n",
    "\n",
    "    while batch_size > max_batch_size:\n",
    "        batch_tokens = batch_tokens[:batch_size // 2]  # Halve the batch size\n",
    "        batch_size = len(batch_tokens)\n",
    "\n",
    "    batch_padded_tokens = [tokens + [tokenizer.pad_token_id] * (max_len - len(tokens))\n",
    "                           for tokens in batch_tokens]\n",
    "\n",
    "    tokens_tensor = torch.tensor(batch_padded_tokens)\n",
    "    with torch.no_grad():\n",
    "        output = model(tokens_tensor)\n",
    "        embeddings = output.last_hidden_state\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef250831",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_embeddings(max_length,batch_size,tokens,model,tokenizer):\n",
    "\n",
    "    embedding_trans = []\n",
    "    for i in tqdm(range(0, len(tokens), batch_size), \"Embedding\", colour= \"green\"):\n",
    "        batch_token = tokens[i : i+batch_size]\n",
    "        embedding_trans.extend(text_embedding2(batch_token,max_length,model,tokenizer))\n",
    "\n",
    "    return embedding_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5aaa4067",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:   0%|\u001b[32m          \u001b[0m| 0/3125 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "Embedding: 100%|\u001b[32m██████████\u001b[0m| 3125/3125 [01:33<00:00, 33.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------English embededding done -------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|\u001b[32m██████████\u001b[0m| 3125/3125 [01:34<00:00, 32.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------French embededding done -------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "english_tokens = [bert_tiny_tokenizer.encode(text,add_special_tokens = True,padding='max_length',max_length=104) for text in eng_sent]\n",
    "french_token = [bert_tiny_tokenizer.encode(text,add_special_tokens = True,padding='max_length',max_length=104) for text in french_sent]\n",
    "English_embeddings = get_embeddings(max_length=104,batch_size=32,tokens=english_tokens,model=Bert_tiny_model,tokenizer=bert_tiny_tokenizer)\n",
    "torch.cuda.empty_cache()\n",
    "print('----------------------English embededding done -------------------')\n",
    "French_embeddings = get_embeddings(max_length=104,batch_size=32,tokens=french_token,model=Bert_tiny_model,tokenizer=bert_tiny_tokenizer)\n",
    "print('----------------------French embededding done -------------------')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c864d2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(English_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4a7d0a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 104, 128])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "#english_tensor_stacked = torch.stack(English_embeddings,dim=0)\n",
    "#french_tensor_stacked = torch.stack(French_embeddings)\n",
    "num_batches = len(English_embeddings) // batch_size\n",
    "for i in range(num_batches):\n",
    "    batched_tensor = torch.stack([torch.stack(English_embeddings[i*batch_size:(i+1)*batch_size])]).cuda()\n",
    "    torch.cuda.empty_cache()\n",
    "print(batched_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd5dea80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing 0th batch\n",
      "Computing 100th batch\n",
      "Computing 200th batch\n",
      "Computing 300th batch\n",
      "Computing 400th batch\n",
      "Computing 500th batch\n",
      "Computing 600th batch\n",
      "Computing 700th batch\n",
      "Computing 800th batch\n",
      "Computing 900th batch\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:80] data. DefaultCPUAllocator: not enough memory: you tried to allocate 5324800000 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m     batched_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(batch_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batched_tensor\n\u001b[1;32m---> 42\u001b[0m English_batched_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mget_batched_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mEnglish_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m English_batched_embeddings\u001b[38;5;241m.\u001b[39msize()\n",
      "Cell \u001b[1;32mIn[20], line 39\u001b[0m, in \u001b[0;36mget_batched_embeddings\u001b[1;34m(batch_size, embeddings, steps_per_epoch)\u001b[0m\n\u001b[0;32m     36\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Stack the batches along a new dimension\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m batched_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batched_tensor\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:80] data. DefaultCPUAllocator: not enough memory: you tried to allocate 5324800000 bytes."
     ]
    }
   ],
   "source": [
    "\n",
    "def get_batched_embeddings(batch_size,embeddings,steps_per_epoch):\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    num_batches = len(embeddings) // batch_size\n",
    "    remainder = len(embeddings) % batch_size\n",
    "\n",
    "    # Define a function to process a batch\n",
    "    def process_batch(batch):\n",
    "        return torch.stack(batch)\n",
    "    \n",
    "    max_batch_size = 100  # You can adjust this value based on your GPU memory capacity\n",
    "\n",
    "    # Create a list to store individual batches\n",
    "    batch_list = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        if i%steps_per_epoch == 0:\n",
    "            print(f'Computing {i}th batch')\n",
    "        batch_start = i * batch_size\n",
    "        batch_end = (i + 1) * batch_size\n",
    "        with torch.no_grad():\n",
    "            batch = embeddings[batch_start:batch_end]\n",
    "            while batch_size > max_batch_size:\n",
    "                batch = batch[:batch_size // 2]  # Halve the batch size\n",
    "                batch_size = len(batch)\n",
    "            processed_batch = process_batch(batch)\n",
    "            batch_list.append(processed_batch)\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Handle the remainder\n",
    "    if remainder > 0:\n",
    "        last_batch = embeddings[-remainder:]\n",
    "        processed_last_batch = process_batch(last_batch)\n",
    "        batch_list.append(processed_last_batch)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Stack the batches along a new dimension\n",
    "    batched_tensor = torch.stack(batch_list, dim=0)\n",
    "\n",
    "    return batched_tensor\n",
    "English_batched_embeddings = get_batched_embeddings(100,English_embeddings,100)\n",
    "English_batched_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e87942d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing 0th batch\n",
      "Computing 100th batch\n",
      "Computing 200th batch\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def get_batched_embeddings(batch_size, embeddings, steps_per_epoch):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    num_batches = len(embeddings) // batch_size\n",
    "    remainder = len(embeddings) % batch_size\n",
    "\n",
    "    # Define a function to process a batch\n",
    "    def process_batch(batch):\n",
    "        return torch.stack(batch)\n",
    "\n",
    "    max_batch_size = 100  # You can adjust this value based on your GPU memory capacity\n",
    "\n",
    "    # Create a list to store individual batches\n",
    "    batch_list = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        if i % steps_per_epoch == 0:\n",
    "            print(f'Computing {i}th batch')\n",
    "\n",
    "        batch_start = i * batch_size\n",
    "        batch_end = (i + 1) * batch_size\n",
    "        with torch.no_grad():\n",
    "            batch = embeddings[batch_start:batch_end]\n",
    "            batch_size = len(batch)  # Reset batch_size for the inner loop\n",
    "\n",
    "            while batch_size > max_batch_size:\n",
    "                sub_batch = batch[:max_batch_size]\n",
    "                processed_sub_batch = process_batch(sub_batch)\n",
    "                batch_list.append(processed_sub_batch)\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                batch = batch[max_batch_size:]\n",
    "                batch_size = len(batch)\n",
    "\n",
    "            processed_batch = process_batch(batch)\n",
    "            batch_list.append(processed_batch)\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Handle the remainder\n",
    "    if remainder > 0:\n",
    "        last_batch = embeddings[-remainder:]\n",
    "        processed_last_batch = process_batch(last_batch)\n",
    "        batch_list.append(processed_last_batch)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Stack the batches along a new dimension\n",
    "    batched_tensor = torch.stack(batch_list, dim=0).to(device)\n",
    "\n",
    "    return batched_tensor\n",
    "English_batched_embeddings = get_batched_embeddings(100,English_embeddings,100)\n",
    "English_batched_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "797db03f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 16, 4])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(16,4)\n",
    "x = torch.tile(x,(32,1,1))\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "75b28676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def positional_encoding(embedding_vectors):\n",
    "\n",
    "    batches,batch_size,max_length, d_model = embedding_vectors.size()\n",
    "    \n",
    "    even_i = torch.arange(0 , d_model , 2).float()\n",
    "    even_denominator = torch.pow(10000, even_i/d_model)\n",
    "    odd_i = torch.arange(1 , d_model , 2).float()\n",
    "    odd_denominator = torch.pow(10000, (odd_i -1)/d_model)\n",
    "\n",
    "    positions = torch.arange(max_length,dtype=torch.float).reshape(max_length,1)\n",
    "\n",
    "    even_pe = torch.sin(positions/even_denominator)\n",
    "    odd_pe = torch.sin(positions/even_denominator)\n",
    "    stacked = torch.stack([even_pe , odd_pe] , dim  = 2)\n",
    "    PE = torch.flatten(stacked,start_dim=1,end_dim=2)\n",
    "    PE = torch.tile(PE,(batch_size,1,1))\n",
    "    test_list=[]\n",
    "\n",
    "    for i in range(batches):\n",
    "        test_list.append(embedding_vectors[i] + PE)\n",
    "    test_list = torch.stack(test_list)\n",
    "    return test_list\n",
    "\n",
    "# Example usage:\n",
    "l = positional_encoding(batched_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f39d4c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1000, 104, 128])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4de5576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = english_tokens[0]\n",
    "len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "896d41cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 104, 128])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_padded_tokens = [sample + [bert_tiny_tokenizer.pad_token_id \n",
    "                                 for i in range(104 - len(sample))]\n",
    "                      ]\n",
    "batch_padded_tokens\n",
    "tokens_tensor = torch.tensor(batch_padded_tokens)\n",
    "with torch.no_grad():\n",
    "    output = Bert_tiny_model(tokens_tensor)\n",
    "    embeddings = output.last_hidden_state\n",
    "embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70a34627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\projects\\\\Machine-Translation\\\\embedding_files'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"embedding_files\"\n",
    "if not os.path.exists(path):\n",
    "   os.makedirs(path)\n",
    "   print(\"The new directory is created!\")\n",
    "json_path = os.path.join(os.getcwd(),path)\n",
    "json_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8ced10",
   "metadata": {},
   "source": [
    "# writing the english embeddings in a json file and uploading it for time saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48a4a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_embedding_dict = {}\n",
    "eng_embeddings_parquet_path = 'eng_embeds.parquet'\n",
    "for i in range(len(eng_embeddings)):\n",
    "    eng_embedding_dict[i] = eng_embeddings[i].tolist()\n",
    "#eng_embedding_dict[1]\n",
    "#eng_embedding_dict_json_object = json.dumps(eng_embedding_dict, indent = 4)\n",
    "eng_embedding_df = pd.DataFrame(eng_embedding_dict)\n",
    "eng_embedding_df.to_parquet(os.path.join(json_path,eng_embeddings_parquet_path))\n",
    "#with open(os.path.join(json_path,eng_embeddings_json_path), \"w\") as outfile:\n",
    "    #outfile.write(eng_embedding_dict_json_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f4dc7a",
   "metadata": {},
   "source": [
    "# writing the french embeddings for time saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74eeea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_embedding_dict = {}\n",
    "french_embeddings_parquet_path = 'french_embeds.parquet'\n",
    "for i in range(len(eng_embeddings)):\n",
    "    french_embedding_dict[i] = fr_embeddings[i].tolist()\n",
    "#eng_embedding_dict[1]\n",
    "#french_embedding_dict_json_object = json.dumps(french_embedding_dict, indent = 4) \n",
    "fr_embedding_df = pd.DataFrame(french_embedding_dict)\n",
    "fr_embedding_df.to_parquet(os.path.join(json_path,french_embeddings_parquet_path))\n",
    "#with open(os.path.join(json_path,french_embeddings_json_path), \"w\") as outfile:\n",
    "    #outfile.write(french_embedding_dict_json_object)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
