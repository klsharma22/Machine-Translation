{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9590096b-77e9-4f1e-ba93-874e043953e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\satya\\anaconda3\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\satya\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import BertTokenizer, BertModel,AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "from flair.embeddings import WordEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tensorflow as tf \n",
    "import os\n",
    "import json\n",
    "import pyarrow\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc59c072-01c4-4848-823c-4e8fb1b39b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_table('fra.txt', header= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad6f2ab3-655e-456d-a96e-e1fd3cb05b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>French</th>\n",
       "      <th>Citation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Marche.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>En route !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Bouge !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229798</th>\n",
       "      <td>Death is something that we're often discourage...</td>\n",
       "      <td>La mort est une chose qu'on nous décourage sou...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229799</th>\n",
       "      <td>Since there are usually multiple websites on a...</td>\n",
       "      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229800</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229801</th>\n",
       "      <td>It may be impossible to get a completely error...</td>\n",
       "      <td>Il est peut-être impossible d'obtenir un Corpu...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229802</th>\n",
       "      <td>I went drinking with one of my boyfriend's fri...</td>\n",
       "      <td>« Je suis allée boire avec un ami de mon compa...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>229803 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  English  \\\n",
       "0                                                     Go.   \n",
       "1                                                     Go.   \n",
       "2                                                     Go.   \n",
       "3                                                     Go.   \n",
       "4                                                     Hi.   \n",
       "...                                                   ...   \n",
       "229798  Death is something that we're often discourage...   \n",
       "229799  Since there are usually multiple websites on a...   \n",
       "229800  If someone who doesn't know your background sa...   \n",
       "229801  It may be impossible to get a completely error...   \n",
       "229802  I went drinking with one of my boyfriend's fri...   \n",
       "\n",
       "                                                   French  \\\n",
       "0                                                    Va !   \n",
       "1                                                 Marche.   \n",
       "2                                              En route !   \n",
       "3                                                 Bouge !   \n",
       "4                                                 Salut !   \n",
       "...                                                   ...   \n",
       "229798  La mort est une chose qu'on nous décourage sou...   \n",
       "229799  Puisqu'il y a de multiples sites web sur chaqu...   \n",
       "229800  Si quelqu'un qui ne connaît pas vos antécédent...   \n",
       "229801  Il est peut-être impossible d'obtenir un Corpu...   \n",
       "229802  « Je suis allée boire avec un ami de mon compa...   \n",
       "\n",
       "                                                 Citation  \n",
       "0       CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "1       CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "2       CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "3       CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "4       CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
       "...                                                   ...  \n",
       "229798  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "229799  CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
       "229800  CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
       "229801  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "229802  CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
       "\n",
       "[229803 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.rename(columns= {0: 'English', 1: 'French', 2: 'Citation'}, inplace= True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a157aba8-9988-4afe-bb17-349d365b3286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It may be impossible to get a completely errorfree corpus due to the nature of this kind of collaborative effort However if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning we might be able to minimize errors'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r\"[!'#$%&()*+,-./:;<=>?@[\\]^`{|}~“”‘’«»‹›„‚–—…·•¡¿’\\\"\\']\"\n",
    "\n",
    "eng_sent, french_sent = [], []\n",
    "\n",
    "for e in range(len(data['English'])):\n",
    "    eng_sent.append(re.sub(pattern, \"\", data['English'][e]))\n",
    "    french_sent.append(re.sub(pattern, \"\", data['French'][e]))\n",
    "eng_sent[229801]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa56c84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229803\n",
      "229803\n"
     ]
    }
   ],
   "source": [
    "print(len(eng_sent))\n",
    "print(len(french_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92f1d228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.653237773223154\n",
      "11.28488313903648\n"
     ]
    }
   ],
   "source": [
    "print(100 - (len(set(eng_sent))/len(eng_sent))*100)\n",
    "print(100 - (len(set(french_sent))/len(french_sent))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b6308ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_sent_unique = list(set(eng_sent))\n",
    "french_sent_unique = list(set(french_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6bafb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "bert_small_tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-small-uncased\")\n",
    "bert_small_model = AutoModel.from_pretrained(\"nlpaueb/legal-bert-small-uncased\")\n",
    "bert_tiny_tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "Bert_tiny_model = AutoModel.from_pretrained(\"prajjwal1/bert-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef250831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embedding(batch_tokens, max_len,model,tokenizer):\n",
    "    batch_padded_tokens = [tokens + [tokenizer.pad_token_id \n",
    "                                 for i in range(max_len - len(tokens))]\n",
    "                      for tokens in batch_tokens]\n",
    "    tokens_tensor = torch.tensor(batch_padded_tokens)\n",
    "    with torch.no_grad():\n",
    "        output = model(tokens_tensor)\n",
    "        embeddings = output.last_hidden_state\n",
    "    return embeddings\n",
    "def get_embeddings(max_length,batch_size,tokens,model,tokenizer):\n",
    "\n",
    "    embedding_trans = []\n",
    "    for i in tqdm(range(0, len(tokens), batch_size), \"Embedding\", colour= \"green\"):\n",
    "        batch_token = tokens[i : i+batch_size]\n",
    "        embedding_trans.extend(text_embedding(batch_token,max_length,model,tokenizer))\n",
    "\n",
    "    return embedding_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5aaa4067",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|\u001b[32m██████████\u001b[0m| 5124/5124 [02:38<00:00, 32.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------English embededding done -------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|\u001b[32m██████████\u001b[0m| 6371/6371 [03:32<00:00, 30.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------French embededding done -------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "english_tokens = [bert_tiny_tokenizer.encode(text,add_special_tokens = True,padding='max_length',max_length=104) for text in eng_sent_unique]\n",
    "french_token = [bert_tiny_tokenizer.encode(text,add_special_tokens = True,padding='max_length',max_length=104) for text in french_sent_unique]\n",
    "English_embeddings = get_embeddings(max_length=104,batch_size=32,tokens=english_tokens,model=Bert_tiny_model,tokenizer=bert_tiny_tokenizer)\n",
    "print('----------------------English embededding done -------------------')\n",
    "French_embeddings = get_embeddings(max_length=104,batch_size=32,tokens=french_token,model=Bert_tiny_model,tokenizer=bert_tiny_tokenizer)\n",
    "print('----------------------French embededding done -------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc493e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([104, 128])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "English_embeddings[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d8a32f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6214,  0.9073, -3.7501,  ..., -1.3641,  0.3438,  2.4600],\n",
      "        [-1.4137,  1.5497,  0.8139,  ..., -0.9379, -0.5408,  1.7873],\n",
      "        [-1.6063,  0.1163,  0.6456,  ..., -0.0927, -0.7592,  2.2856],\n",
      "        ...,\n",
      "        [-0.3106,  0.9202, -1.2334,  ..., -1.2643,  0.5254,  2.5815],\n",
      "        [ 0.1219,  0.0890, -0.3993,  ..., -1.3304,  0.7016,  2.7764],\n",
      "        [-0.2284, -0.6628,  0.1658,  ..., -1.2989,  0.7516,  2.8825]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def positional_encoding(embedding_vector, position=None, max_length=104):\n",
    "    \"\"\"\n",
    "    Add positional encoding to an embedding vector.\n",
    "\n",
    "    Parameters:\n",
    "    - embedding_vector: The input embedding vector.\n",
    "    - position: The position of the element in the sequence. If None, positional encoding will be added for all positions.\n",
    "    - max_length: The maximum length of the sequence.\n",
    "\n",
    "    Returns:\n",
    "    - The embedding vector with added positional encoding.\n",
    "    \"\"\"\n",
    "\n",
    "    d_model = embedding_vector.size()[1]\n",
    "    \n",
    "    if position is None:\n",
    "        position = np.arange(0, max_length).reshape(-1, 1)\n",
    "    \n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "    pos_enc = np.zeros((max_length, d_model))\n",
    "    pos_enc[:, 0::2] = np.sin(position * div_term)\n",
    "    pos_enc[:, 1::2] = np.cos(position * div_term)\n",
    "    \n",
    "    return embedding_vector + pos_enc\n",
    "\n",
    "# Example usage:\n",
    "#embedding_vector = np.random.rand(300)  # Replace with your actual embedding vector\n",
    "position = np.arange(0, len(English_embeddings[0])).reshape(-1, 1)\n",
    "embedding_vector_with_pos_enc = positional_encoding(English_embeddings[0], position)\n",
    "\n",
    "print(embedding_vector_with_pos_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4de5576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = english_tokens[0]\n",
    "len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "896d41cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 104, 128])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_padded_tokens = [sample + [bert_tiny_tokenizer.pad_token_id \n",
    "                                 for i in range(104 - len(sample))]\n",
    "                      ]\n",
    "batch_padded_tokens\n",
    "tokens_tensor = torch.tensor(batch_padded_tokens)\n",
    "with torch.no_grad():\n",
    "    output = Bert_tiny_model(tokens_tensor)\n",
    "    embeddings = output.last_hidden_state\n",
    "embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20fc91ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(English_embeddings[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab076056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(French_embeddings[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70a34627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\projects\\\\Machine-Translation\\\\embedding_files'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"embedding_files\"\n",
    "if not os.path.exists(path):\n",
    "   os.makedirs(path)\n",
    "   print(\"The new directory is created!\")\n",
    "json_path = os.path.join(os.getcwd(),path)\n",
    "json_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8ced10",
   "metadata": {},
   "source": [
    "# writing the english embeddings in a json file and uploading it for time saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48a4a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_embedding_dict = {}\n",
    "eng_embeddings_parquet_path = 'eng_embeds.parquet'\n",
    "for i in range(len(eng_embeddings)):\n",
    "    eng_embedding_dict[i] = eng_embeddings[i].tolist()\n",
    "#eng_embedding_dict[1]\n",
    "#eng_embedding_dict_json_object = json.dumps(eng_embedding_dict, indent = 4)\n",
    "eng_embedding_df = pd.DataFrame(eng_embedding_dict)\n",
    "eng_embedding_df.to_parquet(os.path.join(json_path,eng_embeddings_parquet_path))\n",
    "#with open(os.path.join(json_path,eng_embeddings_json_path), \"w\") as outfile:\n",
    "    #outfile.write(eng_embedding_dict_json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c384829",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_embeddings = sent_model.encode(french_sent_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f4dc7a",
   "metadata": {},
   "source": [
    "# writing the french embeddings for time saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74eeea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_embedding_dict = {}\n",
    "french_embeddings_parquet_path = 'french_embeds.parquet'\n",
    "for i in range(len(eng_embeddings)):\n",
    "    french_embedding_dict[i] = fr_embeddings[i].tolist()\n",
    "#eng_embedding_dict[1]\n",
    "#french_embedding_dict_json_object = json.dumps(french_embedding_dict, indent = 4) \n",
    "fr_embedding_df = pd.DataFrame(french_embedding_dict)\n",
    "fr_embedding_df.to_parquet(os.path.join(json_path,french_embeddings_parquet_path))\n",
    "#with open(os.path.join(json_path,french_embeddings_json_path), \"w\") as outfile:\n",
    "    #outfile.write(french_embedding_dict_json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a724a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_token = [tokenizer.encode(text, add_special_tokens= True) for text in eng_sent_unique]\n",
    "fren_token = [tokenizer.encode(text, add_special_tokens= True) for text in french_sent_unique]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be72bb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 440M/440M [00:34<00:00, 12.7MB/s] \n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a85ca07",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
