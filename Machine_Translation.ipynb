{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9590096b-77e9-4f1e-ba93-874e043953e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch , gc\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import BertTokenizer, BertModel,AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf \n",
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9e2c42f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_name\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/cuda/__init__.py:419\u001b[0m, in \u001b[0;36mget_device_name\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_name\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    408\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Gets the name of a device.\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \n\u001b[1;32m    410\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m        str: the name of the device\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/cuda/__init__.py:449\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_properties\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[1;32m    440\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Gets the properties of a device.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \n\u001b[1;32m    442\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[1;32m    450\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/cuda/__init__.py:289\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6f2ab3-655e-456d-a96e-e1fd3cb05b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>French</th>\n",
       "      <th>Citation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Marche.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>En route !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Bouge !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>We've made it.</td>\n",
       "      <td>Nous avons réussi.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>We've made it.</td>\n",
       "      <td>Nous y sommes parvenus.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>We've matured.</td>\n",
       "      <td>Nous avons mûri.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>We've no time.</td>\n",
       "      <td>Nous n'avons pas le temps.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>We've refused.</td>\n",
       "      <td>Nous avons refusé.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             English                      French  \\\n",
       "0                Go.                        Va !   \n",
       "1                Go.                     Marche.   \n",
       "2                Go.                  En route !   \n",
       "3                Go.                     Bouge !   \n",
       "4                Hi.                     Salut !   \n",
       "...              ...                         ...   \n",
       "9995  We've made it.          Nous avons réussi.   \n",
       "9996  We've made it.     Nous y sommes parvenus.   \n",
       "9997  We've matured.            Nous avons mûri.   \n",
       "9998  We've no time.  Nous n'avons pas le temps.   \n",
       "9999  We've refused.          Nous avons refusé.   \n",
       "\n",
       "                                               Citation  \n",
       "0     CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "1     CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "2     CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "3     CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "4     CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
       "...                                                 ...  \n",
       "9995  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "9996  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "9997  CC-BY 2.0 (France) Attribution: tatoeba.org #7...  \n",
       "9998  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "9999  CC-BY 2.0 (France) Attribution: tatoeba.org #7...  \n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_table('fra.txt', header= None)\n",
    "data.rename(columns= {0: 'English', 1: 'French', 2: 'Citation'}, inplace= True)\n",
    "data = data[:10000]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a157aba8-9988-4afe-bb17-349d365b3286",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[!\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#$\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m&()*+,-./:;<=>?@[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m]^`\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m|}~“”‘’«»‹›„‚–—…·•¡¿’\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m eng_sent, french_sent \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mdata\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnglish\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n\u001b[1;32m      6\u001b[0m     eng_sent\u001b[38;5;241m.\u001b[39mappend(re\u001b[38;5;241m.\u001b[39msub(pattern, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnglish\u001b[39m\u001b[38;5;124m'\u001b[39m][e]))\n\u001b[1;32m      7\u001b[0m     french_sent\u001b[38;5;241m.\u001b[39mappend(re\u001b[38;5;241m.\u001b[39msub(pattern, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrench\u001b[39m\u001b[38;5;124m'\u001b[39m][e]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "pattern = r\"[!'#$%&()*+,-./:;<=>?@[\\]^`{|}~“”‘’«»‹›„‚–—…·•¡¿’\\\"\\']\"\n",
    "\n",
    "eng_sent, french_sent = [], []\n",
    "\n",
    "for e in range(len(data['English'])):\n",
    "    eng_sent.append(re.sub(pattern, \"\", data['English'][e]))\n",
    "    french_sent.append(re.sub(pattern, \"\", data['French'][e]))\n",
    "#eng_sent[229801]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa56c84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(eng_sent))\n",
    "print(len(french_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6bafb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bert_tiny_tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "Bert_tiny_model = AutoModel.from_pretrained(\"prajjwal1/bert-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3d71d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bert_tiny_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e3aa447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embedding2(batch_tokens, max_len, model, tokenizer):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Reduce the batch size if the input is too large for GPU memory\n",
    "    batch_size = len(batch_tokens)\n",
    "    max_batch_size = 32  # You can adjust this value based on your GPU memory capacity\n",
    "\n",
    "    while batch_size > max_batch_size:\n",
    "        batch_tokens = batch_tokens[:batch_size // 2]  # Halve the batch size\n",
    "        batch_size = len(batch_tokens)\n",
    "\n",
    "    batch_padded_tokens = [tokens + [tokenizer.pad_token_id] * (max_len - len(tokens))\n",
    "                           for tokens in batch_tokens]\n",
    "\n",
    "    tokens_tensor = torch.tensor(batch_padded_tokens).to(device=device)\n",
    "    with torch.no_grad():\n",
    "        output = model(tokens_tensor)\n",
    "        embeddings = output.last_hidden_state\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef250831",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_embeddings(max_length,batch_size,tokens,model,tokenizer):\n",
    "\n",
    "    embedding_trans = []\n",
    "    for i in tqdm(range(0, len(tokens), batch_size), \"Embedding\", colour= \"green\"):\n",
    "        batch_token = tokens[i : i+batch_size]\n",
    "        embedding_trans.extend(text_embedding2(batch_token,max_length,model,tokenizer))\n",
    "\n",
    "    return embedding_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aaa4067",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------English embededding done -------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m English_embeddings \u001b[38;5;241m=\u001b[39m get_embeddings(max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m104\u001b[39m,batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,tokens\u001b[38;5;241m=\u001b[39menglish_tokens,model\u001b[38;5;241m=\u001b[39mBert_tiny_model,tokenizer\u001b[38;5;241m=\u001b[39mbert_tiny_tokenizer)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m----------------------English embededding done -------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m English_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEnglish_embeddings\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m      8\u001b[0m French_embeddings \u001b[38;5;241m=\u001b[39m get_embeddings(max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m104\u001b[39m,batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,tokens\u001b[38;5;241m=\u001b[39mfrench_token,model\u001b[38;5;241m=\u001b[39mBert_tiny_model,tokenizer\u001b[38;5;241m=\u001b[39mbert_tiny_tokenizer)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "english_tokens = [bert_tiny_tokenizer.encode(text,add_special_tokens = True,padding='max_length',max_length=104) for text in eng_sent]\n",
    "french_token = [bert_tiny_tokenizer.encode(text,add_special_tokens = True,padding='max_length',max_length=104) for text in french_sent]\n",
    "English_embeddings = get_embeddings(max_length=104,batch_size=32,tokens=english_tokens,model=Bert_tiny_model,tokenizer=bert_tiny_tokenizer)\n",
    "print('----------------------English embededding done -------------------')\n",
    "English_embeddings = torch.stack(English_embeddings).to(device='cpu')\n",
    "torch.cuda.empty_cache()\n",
    "French_embeddings = get_embeddings(max_length=104,batch_size=32,tokens=french_token,model=Bert_tiny_model,tokenizer=bert_tiny_tokenizer)\n",
    "print('----------------------French embededding done -------------------')\n",
    "French_embeddings = torch.stack(French_embeddings).to(device='cpu')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c864d2e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mEnglish_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "English_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75b28676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def positional_encoding(embedding_vectors,batch_size):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    records,max_length, d_model = embedding_vectors.size()\n",
    "    \n",
    "    even_i = torch.arange(0 , d_model , 2).float()\n",
    "    even_denominator = torch.pow(10000, even_i/d_model)\n",
    "    odd_i = torch.arange(1 , d_model , 2).float()\n",
    "    odd_denominator = torch.pow(10000, (odd_i -1)/d_model)\n",
    "\n",
    "    positions = torch.arange(max_length,dtype=torch.float).reshape(max_length,1)\n",
    "\n",
    "    even_pe = torch.sin(positions/even_denominator)\n",
    "    odd_pe = torch.sin(positions/even_denominator)\n",
    "    stacked = torch.stack([even_pe , odd_pe] , dim  = 2)\n",
    "    PE = torch.flatten(stacked,start_dim=1,end_dim=2)\n",
    "    PE = torch.tile(PE,(batch_size,1,1))\n",
    "    test_list=[]\n",
    "\n",
    "    for i in tqdm(range(0 ,records,batch_size), \"Positional_Encoding\", colour= \"green\"):\n",
    "        batch = embedding_vectors[i:i+batch_size]\n",
    "        test_list.append(batch + PE)\n",
    "    test_list = torch.stack(test_list).to(device=device)\n",
    "    test_list= torch.flatten(test_list,start_dim=0,end_dim=1).to(device=device)\n",
    "    return test_list\n",
    "\n",
    "# Example usage:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28783324",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m English_position_encoded\u001b[38;5;241m=\u001b[39m \u001b[43mpositional_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEnglish_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m English_position_encoded\u001b[38;5;241m=\u001b[39m English_position_encoded\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m, in \u001b[0;36mpositional_encoding\u001b[0;34m(embedding_vectors, batch_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpositional_encoding\u001b[39m(embedding_vectors,batch_size):\n\u001b[1;32m      4\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     records,max_length, d_model \u001b[38;5;241m=\u001b[39m \u001b[43membedding_vectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m()\n\u001b[1;32m      7\u001b[0m     even_i \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m , d_model , \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m      8\u001b[0m     even_denominator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m10000\u001b[39m, even_i\u001b[38;5;241m/\u001b[39md_model)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "English_position_encoded= positional_encoding(English_embeddings,batch_size=50)\n",
    "English_position_encoded= English_position_encoded.to(device='cpu')\n",
    "torch.cuda.empty_cache()\n",
    "French_position_encoded = positional_encoding(French_embeddings,batch_size=50)\n",
    "French_position_encoded= French_position_encoded.to(device='cpu')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51e50e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c5f6791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import Transformer # This is from transformer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d1e2228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODER ACTIVATED\n",
      "tokenization done \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull([max_sequence_length, max_sequence_length] , \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     21\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtriu(mask, diagonal\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Mask initialization for masked attention\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43meng_sent\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfrench_sent\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Machine-Translation/transformer.py:451\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, y, encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m    444\u001b[0m             x, \n\u001b[1;32m    445\u001b[0m             y, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    448\u001b[0m             decoder_cross_attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    449\u001b[0m             ): \u001b[38;5;66;03m# x, y are batch of sentences\u001b[39;00m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mENCODER ACTIVATED\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 451\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_self_attention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mENCODER COMPLETED\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMEMORY AFTER ENCODER ACTION\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_summary(abbreviated\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)))\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Machine-Translation/transformer.py:301\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x, self_attention_mask)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, self_attention_mask):\n\u001b[0;32m--> 301\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentence_embedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMEMORY USAGE AFTER ENCODING\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_summary())\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n",
      "File \u001b[0;32m~/Desktop/Machine-Translation/transformer.py:138\u001b[0m, in \u001b[0;36mSentenceEmbedding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    136\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize(x)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenization done\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 138\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_encoder\u001b[38;5;241m.\u001b[39mpositional_encoding(x)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSENTENCE EMBEDDING DONE\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,x\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m~/Desktop/Machine-Translation/transformer.py:114\u001b[0m, in \u001b[0;36mEmbedding.get_embeddings\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    112\u001b[0m     batch_token \u001b[38;5;241m=\u001b[39m tokens[i : i\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size]\n\u001b[1;32m    113\u001b[0m     embedding_trans\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_embedding(batch_token))\n\u001b[0;32m--> 114\u001b[0m embedding_trans \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_trans\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    115\u001b[0m embedding_trans \u001b[38;5;241m=\u001b[39m embedding_trans\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    116\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb=32'\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "drop_prob = 0.1\n",
    "batch_size = 30\n",
    "max_sequence_length = 104\n",
    "ffn_hidden = 2048\n",
    "num_layers_encoder = 1\n",
    "num_layers_decoder = 1\n",
    "transformer = Transformer(d_model = 128,\n",
    "                    ffn_hidden = 256,\n",
    "                    num_heads = 8,\n",
    "                    drop_prob = 0.1,\n",
    "                    max_sequence_length = 104,\n",
    "                    num_layers_encoder = 2,\n",
    "                    num_layers_decoder = 1,\n",
    "                    batch_size=50\n",
    "                    )\n",
    "\n",
    "mask = torch.full([max_sequence_length, max_sequence_length] , float('-inf'))\n",
    "mask = torch.triu(mask, diagonal=1) # Mask initialization for masked attention\n",
    "decoder_output = transformer(eng_sent[:1000],french_sent[:1000],mask)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729e17ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   3391 MiB |   3442 MiB |  16292 MiB |  12901 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   3391 MiB |   3442 MiB |  16292 MiB |  12901 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   3387 MiB |   3438 MiB |  16151 MiB |  12763 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   3452 MiB |   3504 MiB |   9870 MiB |   6418 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  62160 KiB | 151938 KiB |   6978 MiB |   6917 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     184    |     188    |    2887    |    2703    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     184    |     188    |    2887    |    2703    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      44    |      45    |     141    |      97    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      35    |      36    |    1664    |    1629    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.memory_summary(abbreviated=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70a34627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new directory is created!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/klsharma22/Desktop/Machine-Translation/encoded_files'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"encoded_files\"\n",
    "if not os.path.exists(path):\n",
    "   os.makedirs(path)\n",
    "   print(\"The new directory is created!\")\n",
    "json_path = os.path.join(os.getcwd(),path)\n",
    "json_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8ced10",
   "metadata": {},
   "source": [
    "# writing the english embeddings in a json file and uploading it for time saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48a4a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_embedding_dict = {}\n",
    "eng_embeddings_parquet_path = 'eng_embeds.parquet'\n",
    "for i in range(len(eng_embeddings)):\n",
    "    eng_embedding_dict[i] = eng_embeddings[i].tolist()\n",
    "#eng_embedding_dict[1]\n",
    "#eng_embedding_dict_json_object = json.dumps(eng_embedding_dict, indent = 4)\n",
    "eng_embedding_df = pd.DataFrame(eng_embedding_dict)\n",
    "eng_embedding_df.to_parquet(os.path.join(json_path,eng_embeddings_parquet_path))\n",
    "#with open(os.path.join(json_path,eng_embeddings_json_path), \"w\") as outfile:\n",
    "    #outfile.write(eng_embedding_dict_json_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f4dc7a",
   "metadata": {},
   "source": [
    "# writing the french embeddings for time saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74eeea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_embedding_dict = {}\n",
    "french_embeddings_parquet_path = 'french_embeds.parquet'\n",
    "for i in range(len(eng_embeddings)):\n",
    "    french_embedding_dict[i] = fr_embeddings[i].tolist()\n",
    "#eng_embedding_dict[1]\n",
    "#french_embedding_dict_json_object = json.dumps(french_embedding_dict, indent = 4) \n",
    "fr_embedding_df = pd.DataFrame(french_embedding_dict)\n",
    "fr_embedding_df.to_parquet(os.path.join(json_path,french_embeddings_parquet_path))\n",
    "#with open(os.path.join(json_path,french_embeddings_json_path), \"w\") as outfile:\n",
    "    #outfile.write(french_embedding_dict_json_object)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
