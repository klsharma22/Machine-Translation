{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9590096b-77e9-4f1e-ba93-874e043953e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\satya\\anaconda3\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch , gc\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import BertTokenizer, BertModel,AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf \n",
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9e2c42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1650 Ti'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad6f2ab3-655e-456d-a96e-e1fd3cb05b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>French</th>\n",
       "      <th>Citation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Marche.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>En route !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Bouge !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I'm really in a bind.</td>\n",
       "      <td>Je suis vraiment dans le pétrin.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>I'm really surprised.</td>\n",
       "      <td>Je suis très étonné.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I'm right behind Tom.</td>\n",
       "      <td>Je suis juste derrière Tom.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm right behind you.</td>\n",
       "      <td>Je suis juste derrière toi.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>I'm right behind you.</td>\n",
       "      <td>Je suis juste derrière vous.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     English                            French  \\\n",
       "0                        Go.                              Va !   \n",
       "1                        Go.                           Marche.   \n",
       "2                        Go.                        En route !   \n",
       "3                        Go.                           Bouge !   \n",
       "4                        Hi.                           Salut !   \n",
       "...                      ...                               ...   \n",
       "49995  I'm really in a bind.  Je suis vraiment dans le pétrin.   \n",
       "49996  I'm really surprised.              Je suis très étonné.   \n",
       "49997  I'm right behind Tom.       Je suis juste derrière Tom.   \n",
       "49998  I'm right behind you.       Je suis juste derrière toi.   \n",
       "49999  I'm right behind you.      Je suis juste derrière vous.   \n",
       "\n",
       "                                                Citation  \n",
       "0      CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "1      CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "2      CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "3      CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "4      CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
       "...                                                  ...  \n",
       "49995  CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
       "49996  CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
       "49997  CC-BY 2.0 (France) Attribution: tatoeba.org #6...  \n",
       "49998  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "49999  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "\n",
       "[50000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_table('fra.txt', header= None)\n",
    "data.rename(columns= {0: 'English', 1: 'French', 2: 'Citation'}, inplace= True)\n",
    "data = data[:50000]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a157aba8-9988-4afe-bb17-349d365b3286",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"[!'#$%&()*+,-./:;<=>?@[\\]^`{|}~“”‘’«»‹›„‚–—…·•¡¿’\\\"\\']\"\n",
    "\n",
    "eng_sent, french_sent = [], []\n",
    "\n",
    "for e in range(len(data['English'])):\n",
    "    eng_sent.append(re.sub(pattern, \"\", data['English'][e]))\n",
    "    french_sent.append(re.sub(pattern, \"\", data['French'][e]))\n",
    "#eng_sent[229801]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa56c84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(len(eng_sent))\n",
    "print(len(french_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6bafb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bert_tiny_tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "Bert_tiny_model = AutoModel.from_pretrained(\"prajjwal1/bert-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3d71d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bert_tiny_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e3aa447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embedding2(batch_tokens, max_len, model, tokenizer):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Reduce the batch size if the input is too large for GPU memory\n",
    "    batch_size = len(batch_tokens)\n",
    "    max_batch_size = 32  # You can adjust this value based on your GPU memory capacity\n",
    "\n",
    "    while batch_size > max_batch_size:\n",
    "        batch_tokens = batch_tokens[:batch_size // 2]  # Halve the batch size\n",
    "        batch_size = len(batch_tokens)\n",
    "\n",
    "    batch_padded_tokens = [tokens + [tokenizer.pad_token_id] * (max_len - len(tokens))\n",
    "                           for tokens in batch_tokens]\n",
    "\n",
    "    tokens_tensor = torch.tensor(batch_padded_tokens).to(device=device)\n",
    "    with torch.no_grad():\n",
    "        output = model(tokens_tensor)\n",
    "        embeddings = output.last_hidden_state\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef250831",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_embeddings(max_length,batch_size,tokens,model,tokenizer):\n",
    "\n",
    "    embedding_trans = []\n",
    "    for i in tqdm(range(0, len(tokens), batch_size), \"Embedding\", colour= \"green\"):\n",
    "        batch_token = tokens[i : i+batch_size]\n",
    "        embedding_trans.extend(text_embedding2(batch_token,max_length,model,tokenizer))\n",
    "\n",
    "    return embedding_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5aaa4067",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|\u001b[32m██████████\u001b[0m| 1563/1563 [00:10<00:00, 151.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------English embededding done -------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|\u001b[32m██████████\u001b[0m| 1563/1563 [00:10<00:00, 154.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------French embededding done -------------------\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "english_tokens = [bert_tiny_tokenizer.encode(text,add_special_tokens = True,padding='max_length',max_length=104) for text in eng_sent]\n",
    "french_token = [bert_tiny_tokenizer.encode(text,add_special_tokens = True,padding='max_length',max_length=104) for text in french_sent]\n",
    "English_embeddings = get_embeddings(max_length=104,batch_size=32,tokens=english_tokens,model=Bert_tiny_model,tokenizer=bert_tiny_tokenizer)\n",
    "print('----------------------English embededding done -------------------')\n",
    "English_embeddings = torch.stack(English_embeddings).to(device='cpu')\n",
    "torch.cuda.empty_cache()\n",
    "French_embeddings = get_embeddings(max_length=104,batch_size=32,tokens=french_token,model=Bert_tiny_model,tokenizer=bert_tiny_tokenizer)\n",
    "print('----------------------French embededding done -------------------')\n",
    "French_embeddings = torch.stack(French_embeddings).to(device='cpu')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb64fe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c864d2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 104, 128])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "English_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75b28676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def positional_encoding(embedding_vectors,batch_size):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    records,max_length, d_model = embedding_vectors.size()\n",
    "    \n",
    "    even_i = torch.arange(0 , d_model , 2).float()\n",
    "    even_denominator = torch.pow(10000, even_i/d_model)\n",
    "    odd_i = torch.arange(1 , d_model , 2).float()\n",
    "    odd_denominator = torch.pow(10000, (odd_i -1)/d_model)\n",
    "\n",
    "    positions = torch.arange(max_length,dtype=torch.float).reshape(max_length,1)\n",
    "\n",
    "    even_pe = torch.sin(positions/even_denominator)\n",
    "    odd_pe = torch.sin(positions/even_denominator)\n",
    "    stacked = torch.stack([even_pe , odd_pe] , dim  = 2)\n",
    "    PE = torch.flatten(stacked,start_dim=1,end_dim=2)\n",
    "    PE = torch.tile(PE,(batch_size,1,1))\n",
    "    test_list=[]\n",
    "\n",
    "    for i in tqdm(range(0 ,records,batch_size), \"Positional_Encoding\", colour= \"green\"):\n",
    "        batch = embedding_vectors[i:i+batch_size]\n",
    "        test_list.append(batch + PE)\n",
    "    test_list = torch.stack(test_list).to(device=device)\n",
    "    test_list= torch.flatten(test_list,start_dim=0,end_dim=1).to(device=device)\n",
    "    return test_list\n",
    "\n",
    "# Example usage:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28783324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Positional_Encoding:   0%|\u001b[32m          \u001b[0m| 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Positional_Encoding: 100%|\u001b[32m██████████\u001b[0m| 1000/1000 [00:00<00:00, 1235.91it/s]\n",
      "Positional_Encoding: 100%|\u001b[32m██████████\u001b[0m| 1000/1000 [00:01<00:00, 585.59it/s]\n"
     ]
    }
   ],
   "source": [
    "English_position_encoded= positional_encoding(English_embeddings,batch_size=50)\n",
    "English_position_encoded= English_position_encoded.to(device='cpu')\n",
    "torch.cuda.empty_cache()\n",
    "French_position_encoded = positional_encoding(French_embeddings,batch_size=50)\n",
    "French_position_encoded= French_position_encoded.to(device='cpu')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51e50e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70a34627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\projects\\\\Machine-Translation\\\\embedding_files'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"embedding_files\"\n",
    "if not os.path.exists(path):\n",
    "   os.makedirs(path)\n",
    "   print(\"The new directory is created!\")\n",
    "json_path = os.path.join(os.getcwd(),path)\n",
    "json_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8ced10",
   "metadata": {},
   "source": [
    "# writing the english embeddings in a json file and uploading it for time saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48a4a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_embedding_dict = {}\n",
    "eng_embeddings_parquet_path = 'eng_embeds.parquet'\n",
    "for i in range(len(eng_embeddings)):\n",
    "    eng_embedding_dict[i] = eng_embeddings[i].tolist()\n",
    "#eng_embedding_dict[1]\n",
    "#eng_embedding_dict_json_object = json.dumps(eng_embedding_dict, indent = 4)\n",
    "eng_embedding_df = pd.DataFrame(eng_embedding_dict)\n",
    "eng_embedding_df.to_parquet(os.path.join(json_path,eng_embeddings_parquet_path))\n",
    "#with open(os.path.join(json_path,eng_embeddings_json_path), \"w\") as outfile:\n",
    "    #outfile.write(eng_embedding_dict_json_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f4dc7a",
   "metadata": {},
   "source": [
    "# writing the french embeddings for time saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74eeea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_embedding_dict = {}\n",
    "french_embeddings_parquet_path = 'french_embeds.parquet'\n",
    "for i in range(len(eng_embeddings)):\n",
    "    french_embedding_dict[i] = fr_embeddings[i].tolist()\n",
    "#eng_embedding_dict[1]\n",
    "#french_embedding_dict_json_object = json.dumps(french_embedding_dict, indent = 4) \n",
    "fr_embedding_df = pd.DataFrame(french_embedding_dict)\n",
    "fr_embedding_df.to_parquet(os.path.join(json_path,french_embeddings_parquet_path))\n",
    "#with open(os.path.join(json_path,french_embeddings_json_path), \"w\") as outfile:\n",
    "    #outfile.write(french_embedding_dict_json_object)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
