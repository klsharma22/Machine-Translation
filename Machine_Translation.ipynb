{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9590096b-77e9-4f1e-ba93-874e043953e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch , gc\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import BertTokenizer, BertModel,AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf \n",
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9e2c42f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_name\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/cuda/__init__.py:419\u001b[0m, in \u001b[0;36mget_device_name\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_name\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    408\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Gets the name of a device.\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \n\u001b[1;32m    410\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m        str: the name of the device\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/cuda/__init__.py:449\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_properties\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[1;32m    440\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Gets the properties of a device.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \n\u001b[1;32m    442\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[1;32m    450\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/cuda/__init__.py:289\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad6f2ab3-655e-456d-a96e-e1fd3cb05b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>French</th>\n",
       "      <th>Citation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Marche.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>En route !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Bouge !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>We've made it.</td>\n",
       "      <td>Nous avons réussi.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>We've made it.</td>\n",
       "      <td>Nous y sommes parvenus.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>We've matured.</td>\n",
       "      <td>Nous avons mûri.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>We've no time.</td>\n",
       "      <td>Nous n'avons pas le temps.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>We've refused.</td>\n",
       "      <td>Nous avons refusé.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             English                      French  \\\n",
       "0                Go.                        Va !   \n",
       "1                Go.                     Marche.   \n",
       "2                Go.                  En route !   \n",
       "3                Go.                     Bouge !   \n",
       "4                Hi.                     Salut !   \n",
       "...              ...                         ...   \n",
       "9995  We've made it.          Nous avons réussi.   \n",
       "9996  We've made it.     Nous y sommes parvenus.   \n",
       "9997  We've matured.            Nous avons mûri.   \n",
       "9998  We've no time.  Nous n'avons pas le temps.   \n",
       "9999  We've refused.          Nous avons refusé.   \n",
       "\n",
       "                                               Citation  \n",
       "0     CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "1     CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "2     CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "3     CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "4     CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
       "...                                                 ...  \n",
       "9995  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "9996  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "9997  CC-BY 2.0 (France) Attribution: tatoeba.org #7...  \n",
       "9998  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "9999  CC-BY 2.0 (France) Attribution: tatoeba.org #7...  \n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_table('fra.txt', header= None)\n",
    "data.rename(columns= {0: 'English', 1: 'French', 2: 'Citation'}, inplace= True)\n",
    "data = data[:10000]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a157aba8-9988-4afe-bb17-349d365b3286",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"[!'#$%&()*+,-./:;<=>?@[\\]^`{|}~“”‘’«»‹›„‚–—…·•¡¿’\\\"\\']\"\n",
    "\n",
    "eng_sent, french_sent = [], []\n",
    "\n",
    "for e in range(len(data['English'])):\n",
    "    eng_sent.append(re.sub(pattern, \"\", data['English'][e]))\n",
    "    french_sent.append(re.sub(pattern, \"\", data['French'][e]))\n",
    "#eng_sent[229801]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa56c84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(eng_sent))\n",
    "print(len(french_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6bafb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bert_tiny_tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "Bert_tiny_model = AutoModel.from_pretrained(\"prajjwal1/bert-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3d71d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bert_tiny_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e3aa447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embedding2(batch_tokens, max_len, model, tokenizer):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Reduce the batch size if the input is too large for GPU memory\n",
    "    batch_size = len(batch_tokens)\n",
    "    max_batch_size = 32  # You can adjust this value based on your GPU memory capacity\n",
    "\n",
    "    while batch_size > max_batch_size:\n",
    "        batch_tokens = batch_tokens[:batch_size // 2]  # Halve the batch size\n",
    "        batch_size = len(batch_tokens)\n",
    "\n",
    "    batch_padded_tokens = [tokens + [tokenizer.pad_token_id] * (max_len - len(tokens))\n",
    "                           for tokens in batch_tokens]\n",
    "\n",
    "    tokens_tensor = torch.tensor(batch_padded_tokens).to(device=device)\n",
    "    with torch.no_grad():\n",
    "        output = model(tokens_tensor)\n",
    "        embeddings = output.last_hidden_state\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef250831",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_embeddings(max_length,batch_size,tokens,model,tokenizer):\n",
    "\n",
    "    embedding_trans = []\n",
    "    for i in tqdm(range(0, len(tokens), batch_size), \"Embedding\", colour= \"green\"):\n",
    "        batch_token = tokens[i : i+batch_size]\n",
    "        embedding_trans.extend(text_embedding2(batch_token,max_length,model,tokenizer))\n",
    "\n",
    "    return embedding_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aaa4067",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m      2\u001b[0m english_tokens \u001b[38;5;241m=\u001b[39m [bert_tiny_tokenizer\u001b[38;5;241m.\u001b[39mencode(text,add_special_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m,max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m104\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m eng_sent]\n\u001b[1;32m      3\u001b[0m french_token \u001b[38;5;241m=\u001b[39m [bert_tiny_tokenizer\u001b[38;5;241m.\u001b[39mencode(text,add_special_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m,max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m104\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m french_sent]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "english_tokens = [bert_tiny_tokenizer.encode(text,add_special_tokens = True,padding='max_length',max_length=104) for text in eng_sent]\n",
    "french_token = [bert_tiny_tokenizer.encode(text,add_special_tokens = True,padding='max_length',max_length=104) for text in french_sent]\n",
    "English_embeddings = get_embeddings(max_length=104,batch_size=32,tokens=english_tokens,model=Bert_tiny_model,tokenizer=bert_tiny_tokenizer)\n",
    "print('----------------------English embededding done -------------------')\n",
    "English_embeddings = torch.stack(English_embeddings).to(device='cpu')\n",
    "torch.cuda.empty_cache()\n",
    "French_embeddings = get_embeddings(max_length=104,batch_size=32,tokens=french_token,model=Bert_tiny_model,tokenizer=bert_tiny_tokenizer)\n",
    "print('----------------------French embededding done -------------------')\n",
    "French_embeddings = torch.stack(French_embeddings).to(device='cpu')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c864d2e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mEnglish_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "English_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75b28676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def positional_encoding(embedding_vectors,batch_size):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    records,max_length, d_model = embedding_vectors.size()\n",
    "    \n",
    "    even_i = torch.arange(0 , d_model , 2).float()\n",
    "    even_denominator = torch.pow(10000, even_i/d_model)\n",
    "    odd_i = torch.arange(1 , d_model , 2).float()\n",
    "    odd_denominator = torch.pow(10000, (odd_i -1)/d_model)\n",
    "\n",
    "    positions = torch.arange(max_length,dtype=torch.float).reshape(max_length,1)\n",
    "\n",
    "    even_pe = torch.sin(positions/even_denominator)\n",
    "    odd_pe = torch.sin(positions/even_denominator)\n",
    "    stacked = torch.stack([even_pe , odd_pe] , dim  = 2)\n",
    "    PE = torch.flatten(stacked,start_dim=1,end_dim=2)\n",
    "    PE = torch.tile(PE,(batch_size,1,1))\n",
    "    test_list=[]\n",
    "\n",
    "    for i in tqdm(range(0 ,records,batch_size), \"Positional_Encoding\", colour= \"green\"):\n",
    "        batch = embedding_vectors[i:i+batch_size]\n",
    "        test_list.append(batch + PE)\n",
    "    test_list = torch.stack(test_list).to(device=device)\n",
    "    test_list= torch.flatten(test_list,start_dim=0,end_dim=1).to(device=device)\n",
    "    return test_list\n",
    "\n",
    "# Example usage:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28783324",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m English_position_encoded\u001b[38;5;241m=\u001b[39m \u001b[43mpositional_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEnglish_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m English_position_encoded\u001b[38;5;241m=\u001b[39m English_position_encoded\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m, in \u001b[0;36mpositional_encoding\u001b[0;34m(embedding_vectors, batch_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpositional_encoding\u001b[39m(embedding_vectors,batch_size):\n\u001b[1;32m      4\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     records,max_length, d_model \u001b[38;5;241m=\u001b[39m \u001b[43membedding_vectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m()\n\u001b[1;32m      7\u001b[0m     even_i \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m , d_model , \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m      8\u001b[0m     even_denominator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m10000\u001b[39m, even_i\u001b[38;5;241m/\u001b[39md_model)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "English_position_encoded= positional_encoding(English_embeddings,batch_size=50)\n",
    "English_position_encoded= English_position_encoded.to(device='cpu')\n",
    "torch.cuda.empty_cache()\n",
    "French_position_encoded = positional_encoding(French_embeddings,batch_size=50)\n",
    "French_position_encoded= French_position_encoded.to(device='cpu')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51e50e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c5f6791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import Transformer # This is from transformer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d1e2228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODER ACTIVATED\n",
      "tokenization done \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|\u001b[32m██████████\u001b[0m| 20/20 [00:00<00:00, 36.84it/s]\n",
      "Positional_Encoding: 100%|\u001b[32m██████████\u001b[0m| 20/20 [00:00<00:00, 3174.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIONAL ENCODING IS DONE \n",
      " torch.Size([1000, 104, 128])\n",
      "SENTENCE EMBEDDING DONE \n",
      " torch.Size([1000, 104, 128])\n",
      "MEMORY USAGE AFTER ENCODING \n",
      "\n",
      "------ ENCODER LAYER NUMBER 1----------\n",
      "ATTENTION 1\n",
      "qkv in cpu\n",
      "qkv in cpu\n",
      "qkv in cpu\n",
      "scaled.size() : torch.Size([1000, 8, 104, 104])\n",
      "values in  cpu\n",
      "Multi headed hattention done , size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      "cpu\n",
      "DROPOUT 1 \n",
      "\n",
      "-ADD AND LAYER NORMALIZATION 1 -\n",
      "Layer normalization done , Size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      " FEED FORWARD NETWORK\n",
      "x feed forward network: torch.Size([1000, 104, 128])\n",
      "-DROPOUT 2 - \n",
      "\n",
      "-ADD AND LAYER NORMALIZATION 2-\n",
      "Layer normalization done , Size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      "------ ENCODER LAYER NUMBER 1----------\n",
      "ATTENTION 1\n",
      "qkv in cpu\n",
      "qkv in cpu\n",
      "qkv in cpu\n",
      "scaled.size() : torch.Size([1000, 8, 104, 104])\n",
      "values in  cpu\n",
      "Multi headed hattention done , size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      "cpu\n",
      "DROPOUT 1 \n",
      "\n",
      "-ADD AND LAYER NORMALIZATION 1 -\n",
      "Layer normalization done , Size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      " FEED FORWARD NETWORK\n",
      "x feed forward network: torch.Size([1000, 104, 128])\n",
      "-DROPOUT 2 - \n",
      "\n",
      "-ADD AND LAYER NORMALIZATION 2-\n",
      "Layer normalization done , Size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      "------ ENCODER LAYER NUMBER 1----------\n",
      "ATTENTION 1\n",
      "qkv in cpu\n",
      "qkv in cpu\n",
      "qkv in cpu\n",
      "scaled.size() : torch.Size([1000, 8, 104, 104])\n",
      "values in  cpu\n",
      "Multi headed hattention done , size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      "cpu\n",
      "DROPOUT 1 \n",
      "\n",
      "-ADD AND LAYER NORMALIZATION 1 -\n",
      "Layer normalization done , Size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      " FEED FORWARD NETWORK\n",
      "x feed forward network: torch.Size([1000, 104, 128])\n",
      "-DROPOUT 2 - \n",
      "\n",
      "-ADD AND LAYER NORMALIZATION 2-\n",
      "Layer normalization done , Size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      "ENCODER COMPLETED\n",
      "MEMORY AFTER ENCODER ACTION \n",
      "\n",
      "DECODER ACTIVATED\n",
      "tokenization done \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|\u001b[32m██████████\u001b[0m| 20/20 [00:00<00:00, 34.81it/s]\n",
      "Positional_Encoding: 100%|\u001b[32m██████████\u001b[0m| 20/20 [00:00<00:00, 4480.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIONAL ENCODING IS DONE \n",
      " torch.Size([1000, 104, 128])\n",
      "SENTENCE EMBEDDING DONE \n",
      " torch.Size([1000, 104, 128])\n",
      "------- DEOCDER LAYER NUMBER 1----------- \n",
      "\n",
      "MASKED SELF ATTENTION\n",
      "qkv in cpu\n",
      "qkv in cpu\n",
      "qkv in cpu\n",
      "scaled.size() : torch.Size([1000, 8, 104, 104])\n",
      "values in  cpu\n",
      "Multi headed hattention done , size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      "DROP OUT 1\n",
      "ADD + LAYER NORMALIZATION 1\n",
      "Layer normalization done , Size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      "CROSS ATTENTION\n",
      "scaled.size() : torch.Size([1000, 8, 104, 104])\n",
      "values in  cpu\n",
      "Cross attention completed size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      "DROP OUT 2 \n",
      "\n",
      "ADD + LAYER NORMALIZATION 2\n",
      "Layer normalization done , Size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      "FEED FORWARD 1\n",
      "x feed forward network: torch.Size([1000, 104, 128])\n",
      "DROP OUT 3 \n",
      "\n",
      "ADD + LAYER NORMALIZATION 3\n",
      "Layer normalization done , Size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      "------- DEOCDER LAYER NUMBER 2----------- \n",
      "\n",
      "MASKED SELF ATTENTION\n",
      "qkv in cpu\n",
      "qkv in cpu\n",
      "qkv in cpu\n",
      "scaled.size() : torch.Size([1000, 8, 104, 104])\n",
      "values in  cpu\n",
      "Multi headed hattention done , size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      "DROP OUT 1\n",
      "ADD + LAYER NORMALIZATION 1\n",
      "Layer normalization done , Size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      "CROSS ATTENTION\n",
      "scaled.size() : torch.Size([1000, 8, 104, 104])\n",
      "values in  cpu\n",
      "Cross attention completed size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      "DROP OUT 2 \n",
      "\n",
      "ADD + LAYER NORMALIZATION 2\n",
      "Layer normalization done , Size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      "FEED FORWARD 1\n",
      "x feed forward network: torch.Size([1000, 104, 128])\n",
      "DROP OUT 3 \n",
      "\n",
      "ADD + LAYER NORMALIZATION 3\n",
      "Layer normalization done , Size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      "------- DEOCDER LAYER NUMBER 3----------- \n",
      "\n",
      "MASKED SELF ATTENTION\n",
      "qkv in cpu\n",
      "qkv in cpu\n",
      "qkv in cpu\n",
      "scaled.size() : torch.Size([1000, 8, 104, 104])\n",
      "values in  cpu\n",
      "Multi headed hattention done , size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      "DROP OUT 1\n",
      "ADD + LAYER NORMALIZATION 1\n",
      "Layer normalization done , Size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      "CROSS ATTENTION\n",
      "scaled.size() : torch.Size([1000, 8, 104, 104])\n",
      "values in  cpu\n",
      "Cross attention completed size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      "DROP OUT 2 \n",
      "\n",
      "ADD + LAYER NORMALIZATION 2\n",
      "Layer normalization done , Size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      "FEED FORWARD 1\n",
      "x feed forward network: torch.Size([1000, 104, 128])\n",
      "DROP OUT 3 \n",
      "\n",
      "ADD + LAYER NORMALIZATION 3\n",
      "Layer normalization done , Size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      "DECODER COMPLETED\n",
      "MEMORY AFTER DECODER ACTION \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb=32'\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "drop_prob = 0.1\n",
    "batch_size = 30\n",
    "max_sequence_length = 104\n",
    "ffn_hidden = 2048\n",
    "num_layers_encoder = 1\n",
    "num_layers_decoder = 1\n",
    "transformer = Transformer(d_model = 128,\n",
    "                    ffn_hidden = 256,\n",
    "                    num_heads = 8,\n",
    "                    drop_prob = 0.1,\n",
    "                    max_sequence_length = 104,\n",
    "                    num_layers_encoder = 3,\n",
    "                    num_layers_decoder = 3,\n",
    "                    batch_size=50\n",
    "                    )\n",
    "\n",
    "mask = torch.full([max_sequence_length, max_sequence_length] , float('-inf'))\n",
    "mask = torch.triu(mask, diagonal=1) # Mask initialization for masked attention\n",
    "decoder_output = transformer(eng_sent[:1000],french_sent[:1000],mask)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729e17ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   3391 MiB |   3442 MiB |  16292 MiB |  12901 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   3391 MiB |   3442 MiB |  16292 MiB |  12901 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   3387 MiB |   3438 MiB |  16151 MiB |  12763 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   3452 MiB |   3504 MiB |   9870 MiB |   6418 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  62160 KiB | 151938 KiB |   6978 MiB |   6917 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     184    |     188    |    2887    |    2703    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     184    |     188    |    2887    |    2703    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      44    |      45    |     141    |      97    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      35    |      36    |    1664    |    1629    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.memory_summary(abbreviated=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70a34627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new directory is created!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/klsharma22/Desktop/Machine-Translation/encoded_files'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"encoded_files\"\n",
    "if not os.path.exists(path):\n",
    "   os.makedirs(path)\n",
    "   print(\"The new directory is created!\")\n",
    "json_path = os.path.join(os.getcwd(),path)\n",
    "json_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8ced10",
   "metadata": {},
   "source": [
    "# writing the english embeddings in a json file and uploading it for time saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48a4a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_embedding_dict = {}\n",
    "eng_embeddings_parquet_path = 'eng_embeds.parquet'\n",
    "for i in range(len(eng_embeddings)):\n",
    "    eng_embedding_dict[i] = eng_embeddings[i].tolist()\n",
    "#eng_embedding_dict[1]\n",
    "#eng_embedding_dict_json_object = json.dumps(eng_embedding_dict, indent = 4)\n",
    "eng_embedding_df = pd.DataFrame(eng_embedding_dict)\n",
    "eng_embedding_df.to_parquet(os.path.join(json_path,eng_embeddings_parquet_path))\n",
    "#with open(os.path.join(json_path,eng_embeddings_json_path), \"w\") as outfile:\n",
    "    #outfile.write(eng_embedding_dict_json_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f4dc7a",
   "metadata": {},
   "source": [
    "# writing the french embeddings for time saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74eeea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_embedding_dict = {}\n",
    "french_embeddings_parquet_path = 'french_embeds.parquet'\n",
    "for i in range(len(eng_embeddings)):\n",
    "    french_embedding_dict[i] = fr_embeddings[i].tolist()\n",
    "#eng_embedding_dict[1]\n",
    "#french_embedding_dict_json_object = json.dumps(french_embedding_dict, indent = 4) \n",
    "fr_embedding_df = pd.DataFrame(french_embedding_dict)\n",
    "fr_embedding_df.to_parquet(os.path.join(json_path,french_embeddings_parquet_path))\n",
    "#with open(os.path.join(json_path,french_embeddings_json_path), \"w\") as outfile:\n",
    "    #outfile.write(french_embedding_dict_json_object)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
