{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9590096b-77e9-4f1e-ba93-874e043953e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\satya\\anaconda3\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch , gc\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import BertTokenizer, BertModel,AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf \n",
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9e2c42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1650 Ti'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad6f2ab3-655e-456d-a96e-e1fd3cb05b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>French</th>\n",
       "      <th>Citation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Marche.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>En route !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Bouge !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>We've made it.</td>\n",
       "      <td>Nous avons réussi.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>We've made it.</td>\n",
       "      <td>Nous y sommes parvenus.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>We've matured.</td>\n",
       "      <td>Nous avons mûri.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>We've no time.</td>\n",
       "      <td>Nous n'avons pas le temps.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>We've refused.</td>\n",
       "      <td>Nous avons refusé.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             English                      French  \\\n",
       "0                Go.                        Va !   \n",
       "1                Go.                     Marche.   \n",
       "2                Go.                  En route !   \n",
       "3                Go.                     Bouge !   \n",
       "4                Hi.                     Salut !   \n",
       "...              ...                         ...   \n",
       "9995  We've made it.          Nous avons réussi.   \n",
       "9996  We've made it.     Nous y sommes parvenus.   \n",
       "9997  We've matured.            Nous avons mûri.   \n",
       "9998  We've no time.  Nous n'avons pas le temps.   \n",
       "9999  We've refused.          Nous avons refusé.   \n",
       "\n",
       "                                               Citation  \n",
       "0     CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "1     CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "2     CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "3     CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "4     CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
       "...                                                 ...  \n",
       "9995  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "9996  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "9997  CC-BY 2.0 (France) Attribution: tatoeba.org #7...  \n",
       "9998  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "9999  CC-BY 2.0 (France) Attribution: tatoeba.org #7...  \n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_table('fra.txt', header= None)\n",
    "data.rename(columns= {0: 'English', 1: 'French', 2: 'Citation'}, inplace= True)\n",
    "data = data[:10000]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a157aba8-9988-4afe-bb17-349d365b3286",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"[!'#$%&()*+,-./:;<=>?@[\\]^`{|}~“”‘’«»‹›„‚–—…·•¡¿’\\\"\\']\"\n",
    "\n",
    "eng_sent, french_sent = [], []\n",
    "\n",
    "for e in range(len(data['English'])):\n",
    "    eng_sent.append(re.sub(pattern, \"\", data['English'][e]))\n",
    "    french_sent.append(re.sub(pattern, \"\", data['French'][e]))\n",
    "#eng_sent[229801]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa56c84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(eng_sent))\n",
    "print(len(french_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6bafb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bert_tiny_tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "Bert_tiny_model = AutoModel.from_pretrained(\"prajjwal1/bert-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3d71d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bert_tiny_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e3aa447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embedding2(batch_tokens, max_len, model, tokenizer):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Reduce the batch size if the input is too large for GPU memory\n",
    "    batch_size = len(batch_tokens)\n",
    "    max_batch_size = 32  # You can adjust this value based on your GPU memory capacity\n",
    "\n",
    "    while batch_size > max_batch_size:\n",
    "        batch_tokens = batch_tokens[:batch_size // 2]  # Halve the batch size\n",
    "        batch_size = len(batch_tokens)\n",
    "\n",
    "    batch_padded_tokens = [tokens + [tokenizer.pad_token_id] * (max_len - len(tokens))\n",
    "                           for tokens in batch_tokens]\n",
    "\n",
    "    tokens_tensor = torch.tensor(batch_padded_tokens).to(device=device)\n",
    "    with torch.no_grad():\n",
    "        output = model(tokens_tensor)\n",
    "        embeddings = output.last_hidden_state\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef250831",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_embeddings(max_length,batch_size,tokens,model,tokenizer):\n",
    "\n",
    "    embedding_trans = []\n",
    "    for i in tqdm(range(0, len(tokens), batch_size), \"Embedding\", colour= \"green\"):\n",
    "        batch_token = tokens[i : i+batch_size]\n",
    "        embedding_trans.extend(text_embedding2(batch_token,max_length,model,tokenizer))\n",
    "\n",
    "    return embedding_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5aaa4067",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:   0%|\u001b[32m          \u001b[0m| 0/1563 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "Embedding: 100%|\u001b[32m██████████\u001b[0m| 1563/1563 [00:14<00:00, 110.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------English embededding done -------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|\u001b[32m██████████\u001b[0m| 1563/1563 [00:12<00:00, 125.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------French embededding done -------------------\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "english_tokens = [bert_tiny_tokenizer.encode(text,add_special_tokens = True,padding='max_length',max_length=104) for text in eng_sent]\n",
    "french_token = [bert_tiny_tokenizer.encode(text,add_special_tokens = True,padding='max_length',max_length=104) for text in french_sent]\n",
    "English_embeddings = get_embeddings(max_length=104,batch_size=32,tokens=english_tokens,model=Bert_tiny_model,tokenizer=bert_tiny_tokenizer)\n",
    "print('----------------------English embededding done -------------------')\n",
    "English_embeddings = torch.stack(English_embeddings).to(device='cpu')\n",
    "torch.cuda.empty_cache()\n",
    "French_embeddings = get_embeddings(max_length=104,batch_size=32,tokens=french_token,model=Bert_tiny_model,tokenizer=bert_tiny_tokenizer)\n",
    "print('----------------------French embededding done -------------------')\n",
    "French_embeddings = torch.stack(French_embeddings).to(device='cpu')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c864d2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 104, 128])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "English_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75b28676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def positional_encoding(embedding_vectors,batch_size):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    records,max_length, d_model = embedding_vectors.size()\n",
    "    \n",
    "    even_i = torch.arange(0 , d_model , 2).float()\n",
    "    even_denominator = torch.pow(10000, even_i/d_model)\n",
    "    odd_i = torch.arange(1 , d_model , 2).float()\n",
    "    odd_denominator = torch.pow(10000, (odd_i -1)/d_model)\n",
    "\n",
    "    positions = torch.arange(max_length,dtype=torch.float).reshape(max_length,1)\n",
    "\n",
    "    even_pe = torch.sin(positions/even_denominator)\n",
    "    odd_pe = torch.sin(positions/even_denominator)\n",
    "    stacked = torch.stack([even_pe , odd_pe] , dim  = 2)\n",
    "    PE = torch.flatten(stacked,start_dim=1,end_dim=2)\n",
    "    PE = torch.tile(PE,(batch_size,1,1))\n",
    "    test_list=[]\n",
    "\n",
    "    for i in tqdm(range(0 ,records,batch_size), \"Positional_Encoding\", colour= \"green\"):\n",
    "        batch = embedding_vectors[i:i+batch_size]\n",
    "        test_list.append(batch + PE)\n",
    "    test_list = torch.stack(test_list).to(device=device)\n",
    "    test_list= torch.flatten(test_list,start_dim=0,end_dim=1).to(device=device)\n",
    "    return test_list\n",
    "\n",
    "# Example usage:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28783324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Positional_Encoding:   0%|\u001b[32m          \u001b[0m| 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Positional_Encoding: 100%|\u001b[32m██████████\u001b[0m| 1000/1000 [00:02<00:00, 441.12it/s]\n",
      "Positional_Encoding: 100%|\u001b[32m██████████\u001b[0m| 1000/1000 [00:00<00:00, 1571.59it/s]\n"
     ]
    }
   ],
   "source": [
    "English_position_encoded= positional_encoding(English_embeddings,batch_size=50)\n",
    "English_position_encoded= English_position_encoded.to(device='cpu')\n",
    "torch.cuda.empty_cache()\n",
    "French_position_encoded = positional_encoding(French_embeddings,batch_size=50)\n",
    "French_position_encoded= French_position_encoded.to(device='cpu')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51e50e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c5f6791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import Transformer # This is from transformer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d1e2228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODER ACTIVATED\n",
      "tokenization done \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:   0%|\u001b[32m          \u001b[0m| 0/20 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "Embedding: 100%|\u001b[32m██████████\u001b[0m| 20/20 [00:02<00:00,  7.89it/s]\n",
      "Positional_Encoding: 100%|\u001b[32m██████████\u001b[0m| 20/20 [00:00<00:00, 1179.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIONAL ENCODING IS DONE \n",
      " torch.Size([1000, 104, 128])\n",
      "SENTENCE EMBEDDING DONE \n",
      " torch.Size([1000, 104, 128])\n",
      "MEMORY USAGE AFTER ENCODING \n",
      " |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  26754 KiB |  78754 KiB |   3371 MiB |   3345 MiB |\n",
      "|       from large pool |  23581 KiB |  75581 KiB |   3359 MiB |   3336 MiB |\n",
      "|       from small pool |   3173 KiB |   3348 KiB |     11 MiB |      8 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  26754 KiB |  78754 KiB |   3371 MiB |   3345 MiB |\n",
      "|       from large pool |  23581 KiB |  75581 KiB |   3359 MiB |   3336 MiB |\n",
      "|       from small pool |   3173 KiB |   3348 KiB |     11 MiB |      8 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  26754 KiB |  78754 KiB |   3284 MiB |   3258 MiB |\n",
      "|       from large pool |  23581 KiB |  75581 KiB |   3273 MiB |   3250 MiB |\n",
      "|       from small pool |   3173 KiB |   3347 KiB |     11 MiB |      8 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  40960 KiB | 114688 KiB | 466944 KiB | 425984 KiB |\n",
      "|       from large pool |  36864 KiB | 110592 KiB | 462848 KiB | 425984 KiB |\n",
      "|       from small pool |   4096 KiB |   4096 KiB |   4096 KiB |      0 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  14205 KiB |  38828 KiB |   3610 MiB |   3596 MiB |\n",
      "|       from large pool |  13283 KiB |  37988 KiB |   3597 MiB |   3584 MiB |\n",
      "|       from small pool |    922 KiB |   2021 KiB |     12 MiB |     11 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      74    |      86    |    1397    |    1323    |\n",
      "|       from large pool |       2    |      11    |     885    |     883    |\n",
      "|       from small pool |      72    |      78    |     512    |     440    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      74    |      86    |    1397    |    1323    |\n",
      "|       from large pool |       2    |      11    |     885    |     883    |\n",
      "|       from small pool |      72    |      78    |     512    |     440    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       4    |       6    |      28    |      24    |\n",
      "|       from large pool |       2    |       4    |      26    |      24    |\n",
      "|       from small pool |       2    |       2    |       2    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       5    |       9    |     647    |     642    |\n",
      "|       from large pool |       3    |       5    |     465    |     462    |\n",
      "|       from small pool |       2    |       4    |     182    |     180    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "------ ENCODER LAYER NUMBER 1----------\n",
      "ATTENTION 1\n",
      "qkv in cpu\n",
      "qkv in cpu\n",
      "qkv in cpu\n",
      "scaled.size() : torch.Size([1000, 8, 104, 104])\n",
      "values in  cuda:0\n",
      "Multi headed hattention done , size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      "cpu\n",
      "DROPOUT 1 \n",
      "\n",
      "-ADD AND LAYER NORMALIZATION 1 -\n",
      "Layer normalization done , Size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      " FEED FORWARD NETWORK\n",
      "x feed forward network: torch.Size([1000, 104, 128])\n",
      "-DROPOUT 2 - \n",
      "\n",
      "-ADD AND LAYER NORMALIZATION 2-\n",
      "Layer normalization done , Size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      "------ ENCODER LAYER NUMBER 1----------\n",
      "ATTENTION 1\n",
      "qkv in cpu\n",
      "qkv in cpu\n",
      "qkv in cpu\n",
      "scaled.size() : torch.Size([1000, 8, 104, 104])\n",
      "values in  cuda:0\n",
      "Multi headed hattention done , size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      "cpu\n",
      "DROPOUT 1 \n",
      "\n",
      "-ADD AND LAYER NORMALIZATION 1 -\n",
      "Layer normalization done , Size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      " FEED FORWARD NETWORK\n",
      "x feed forward network: torch.Size([1000, 104, 128])\n",
      "-DROPOUT 2 - \n",
      "\n",
      "-ADD AND LAYER NORMALIZATION 2-\n",
      "Layer normalization done , Size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      "ENCODER COMPLETED\n",
      "MEMORY AFTER ENCODER ACTION \n",
      " |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   1783 MiB |   1833 MiB |   8633 MiB |   6850 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   1783 MiB |   1833 MiB |   8633 MiB |   6850 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   1781 MiB |   1832 MiB |   8543 MiB |   6762 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   1854 MiB |   1906 MiB |   5298 MiB |   3444 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  72697 KiB | 123462 KiB |   4021 MiB |   3950 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     104    |     105    |    1457    |    1353    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     104    |     105    |    1457    |    1353    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      25    |      26    |      71    |      46    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      22    |      23    |     688    |     666    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "DECODER ACTIVATED\n",
      "tokenization done \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|\u001b[32m██████████\u001b[0m| 20/20 [00:01<00:00, 11.67it/s]\n",
      "Positional_Encoding: 100%|\u001b[32m██████████\u001b[0m| 20/20 [00:00<00:00, 1180.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIONAL ENCODING IS DONE \n",
      " torch.Size([1000, 104, 128])\n",
      "SENTENCE EMBEDDING DONE \n",
      " torch.Size([1000, 104, 128])\n",
      "------- DEOCDER LAYER NUMBER 1----------- \n",
      "\n",
      "MASKED SELF ATTENTION\n",
      "qkv in cpu\n",
      "qkv in cpu\n",
      "qkv in cpu\n",
      "scaled.size() : torch.Size([1000, 8, 104, 104])\n",
      "values in  cuda:0\n",
      "Multi headed hattention done , size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      "DROP OUT 1\n",
      "ADD + LAYER NORMALIZATION 1\n",
      "Layer normalization done , Size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      "CROSS ATTENTION\n",
      "scaled.size() : torch.Size([1000, 8, 104, 104])\n",
      "values in  cuda:0\n",
      "Cross attention completed size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      "DROP OUT 2 \n",
      "\n",
      "ADD + LAYER NORMALIZATION 2\n",
      "Layer normalization done , Size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      "FEED FORWARD 1\n",
      "x feed forward network: torch.Size([1000, 104, 128])\n",
      "DROP OUT 3 \n",
      "\n",
      "ADD + LAYER NORMALIZATION 3\n",
      "Layer normalization done , Size=: torch.Size([1000, 104, 128]) \n",
      "\n",
      "DECODER COMPLETED\n",
      "MEMORY AFTER DECODER ACTION \n",
      " |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   3390 MiB |   3441 MiB |  16830 MiB |  13440 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   3390 MiB |   3441 MiB |  16830 MiB |  13440 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   3387 MiB |   3438 MiB |  16658 MiB |  13271 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   3452 MiB |   3504 MiB |  10218 MiB |   6766 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  62678 KiB | 152456 KiB |   7241 MiB |   7180 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     172    |     173    |    2875    |    2703    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     172    |     173    |    2875    |    2703    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      44    |      45    |     135    |      91    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      37    |    1392    |    1356    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb=32'\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "drop_prob = 0.1\n",
    "batch_size = 30\n",
    "max_sequence_length = 104\n",
    "ffn_hidden = 2048\n",
    "num_layers_encoder = 1\n",
    "num_layers_decoder = 1\n",
    "transformer = Transformer(d_model = 128,\n",
    "                    ffn_hidden = 256,\n",
    "                    num_heads = 8,\n",
    "                    drop_prob = 0.1,\n",
    "                    max_sequence_length = 104,\n",
    "                    num_layers_encoder = 2,\n",
    "                    num_layers_decoder = 1,\n",
    "                    batch_size=50\n",
    "                    )\n",
    "\n",
    "mask = torch.full([max_sequence_length, max_sequence_length] , float('-inf'))\n",
    "mask = torch.triu(mask, diagonal=1) # Mask initialization for masked attention\n",
    "decoder_output = transformer(eng_sent[:1000],french_sent[:1000],mask)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "729e17ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   3391 MiB |   3442 MiB |  16292 MiB |  12901 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   3391 MiB |   3442 MiB |  16292 MiB |  12901 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   3387 MiB |   3438 MiB |  16151 MiB |  12763 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   3452 MiB |   3504 MiB |   9870 MiB |   6418 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  62160 KiB | 151938 KiB |   6978 MiB |   6917 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     184    |     188    |    2887    |    2703    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     184    |     188    |    2887    |    2703    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      44    |      45    |     141    |      97    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      35    |      36    |    1664    |    1629    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.memory_summary(abbreviated=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70a34627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\projects\\\\Machine-Translation\\\\embedding_files'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"encoded_files\"\n",
    "if not os.path.exists(path):\n",
    "   os.makedirs(path)\n",
    "   print(\"The new directory is created!\")\n",
    "json_path = os.path.join(os.getcwd(),path)\n",
    "json_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8ced10",
   "metadata": {},
   "source": [
    "# writing the english embeddings in a json file and uploading it for time saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48a4a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_embedding_dict = {}\n",
    "eng_embeddings_parquet_path = 'eng_embeds.parquet'\n",
    "for i in range(len(eng_embeddings)):\n",
    "    eng_embedding_dict[i] = eng_embeddings[i].tolist()\n",
    "#eng_embedding_dict[1]\n",
    "#eng_embedding_dict_json_object = json.dumps(eng_embedding_dict, indent = 4)\n",
    "eng_embedding_df = pd.DataFrame(eng_embedding_dict)\n",
    "eng_embedding_df.to_parquet(os.path.join(json_path,eng_embeddings_parquet_path))\n",
    "#with open(os.path.join(json_path,eng_embeddings_json_path), \"w\") as outfile:\n",
    "    #outfile.write(eng_embedding_dict_json_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f4dc7a",
   "metadata": {},
   "source": [
    "# writing the french embeddings for time saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74eeea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_embedding_dict = {}\n",
    "french_embeddings_parquet_path = 'french_embeds.parquet'\n",
    "for i in range(len(eng_embeddings)):\n",
    "    french_embedding_dict[i] = fr_embeddings[i].tolist()\n",
    "#eng_embedding_dict[1]\n",
    "#french_embedding_dict_json_object = json.dumps(french_embedding_dict, indent = 4) \n",
    "fr_embedding_df = pd.DataFrame(french_embedding_dict)\n",
    "fr_embedding_df.to_parquet(os.path.join(json_path,french_embeddings_parquet_path))\n",
    "#with open(os.path.join(json_path,french_embeddings_json_path), \"w\") as outfile:\n",
    "    #outfile.write(french_embedding_dict_json_object)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
